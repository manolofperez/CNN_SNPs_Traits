{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgc_VfbydhlW"
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle, choice\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from random import shuffle, choice\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "# define a function to build MLP for the trait data.    \n",
    "def create_mlp(traitstrain, regularizer=None):\n",
    "  model = Sequential()\n",
    "  # first layer, remember to remove bias if you are intercalating with batch normalization. ReLu is the activation (nonlinear) function. L1 regularizes the high-dimensional input with many correlated features and is robust to outliers.\n",
    "  model.add(Dense(150, use_bias=False, input_dim=traitstrain.shape[1], activation=\"relu\", kernel_regularizer=regularizers.l1(0.001)))\n",
    "  # batch normalization.\n",
    "  model.add(BatchNormalization())\n",
    "  # second layer.\n",
    "  model.add(Dense(150, use_bias=False, activation=\"relu\"))\n",
    "  model.add(BatchNormalization())\n",
    "  # third layer.\n",
    "  model.add(Dense(50, activation=\"relu\"))\n",
    "  return model\n",
    "\n",
    "# define a function to build a CNN for the SNP data. \n",
    "def create_cnn(xtest, regularizer=None):\n",
    "  # obtain the input dimensions.\n",
    "  inputShape = (xtest.shape[1], xtest.shape[2])\n",
    "  inputs = Input(shape=inputShape)\n",
    "  x = inputs\n",
    "  # first convolutional layer, remember to remove bias if you are intercalating with batch normalization.\n",
    "  x = Conv1D(250, kernel_size=3, activation='relu', use_bias=False, input_shape=(xtest.shape[1], xtest.shape[2]))(x)\n",
    "  # batch normalization.\n",
    "  x = BatchNormalization()(x)\n",
    "  # second layer.\n",
    "  x = Conv1D(250, kernel_size=3, use_bias=False, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  # third layer.\n",
    "  x = Conv1D(250, kernel_size=3, use_bias=False, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  # pool the CNN outputs.\n",
    "  x = MaxPooling1D(pool_size=3)(x)\n",
    "  # flatten in a single vector.\n",
    "  x = Flatten()(x)\n",
    "  # this part is similar to the MLP, a fully connected neural network. We intercalated with dropout to reduce overfitting.\n",
    "  x = Dense(125, activation='relu')(x)\n",
    "  # dropout.\n",
    "  x = Dropout(0.5)(x)\n",
    "  # second layer of the fully connected neural network.\n",
    "  x = Dense(125, activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  # third layer of the fully connected neural network. This one matches the number of nodes coming out of the MLP.\n",
    "  x = Dense(50, kernel_regularizer=regularizer)(x)\n",
    "  x = Activation(\"relu\")(x)\n",
    "  # Construct the CNN\n",
    "  model = Model(inputs, x)\n",
    "  # Return the CNN\n",
    "  return model\n",
    "\n",
    "# define a function to combine the outputs of the MLP and the CNN.\n",
    "# this was obtained from: https://towardsdatascience.com/neural-networks-ensemble-33f33bea7df3\n",
    "class LinearW(Layer):\n",
    "    def __init__(self):\n",
    "        super(LinearW, self).__init__()    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='name',\n",
    "                    shape=(1,1,len(input_shape)),\n",
    "                    initializer='uniform',\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True)\n",
    "    def call(self, inputs):\n",
    "        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n",
    "        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n",
    "        inputs = [tf.expand_dims(i, -1) for i in inputs]\n",
    "        inputs = Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n",
    "        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n",
    "        # weights sum up to one on last dim\n",
    "        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9nDx2HPbIjP"
   },
   "outputs": [],
   "source": [
    "## define variables that will be used to train all networks.\n",
    "# size of the minibatches containing simulations are passed through the network in each epoch.\n",
    "batch_size = 250\n",
    "# number of training iterations (epochs) for the SNP only and the combined networks.\n",
    "epochs = 100\n",
    "# number of training iterations (epochs) for the traits only networks.\n",
    "epochs_traits = 500\n",
    "# number of scenarios being classified.\n",
    "num_classes = 3\n",
    "\n",
    "# load the traits simulated under the BM model for the 3 scenarios. \n",
    "traits_BM = []\n",
    "traits_BM = np.loadtxt(\"./traits/traits_BM.txt\").reshape(30000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_BM = np.array(traits_BM)\n",
    "\n",
    "# standard scale the continuous (BM) traits\n",
    "scalers = {}\n",
    "for i in range(traits_BM.shape[2]):\n",
    "    scalers[i] = StandardScaler(copy=False)\n",
    "    traits_BM[:, :, i] = scalers[i].fit_transform(traits_BM[:, :, i]) \n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "# transform SNP major alleles in -1 and minor in 1.\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "\n",
    "# create a label vector in the same order as the simulations.\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "# make sure labels, SNP and traits matrices all have the same length.\n",
    "print (len(y), len(X), len(traits_BM))\n",
    "\n",
    "# create a list with the same length of the SNP matrices.\n",
    "shf = list(range(len(X)))\n",
    "# shuffle the list\n",
    "shuffle(shf)\n",
    "\n",
    "# apply the same shuffle to the labels.\n",
    "y = y[shf]\n",
    "# apply the same shuffle to the SNP matrices.\n",
    "X = X[shf]\n",
    "# apply the same shuffle to the traits matrices.\n",
    "traits_BM = traits_BM[shf]\n",
    "\n",
    "# separate the first 75% of labels, SNP and traits matrices as training set. The last 25% are assigned to the test set.\n",
    "ytrain, ytest = y[int(len(y)*.25):], y[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "traits_BM_train, traits_BM_test = traits_BM[int(len(y)*.25):], traits_BM[:int(len(y)*.25)]\n",
    "\n",
    "# convert labels to a categorical matrix of binary values (0 or 1). The number of rows is the length of the input vector (number of simulations) and the number of columns is the number of classes (3 scenarios).\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "# reshape the traits matrices to input them into the MLP\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "cnn = create_cnn(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJgkHTL9Tddn",
    "outputId": "866e837e-0e7e-4f96-e10b-349e252d23bd"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 BM, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model1 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model1.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model1.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model1.layers: print(layer.get_config(), layer.get_weights())\n",
    "\n",
    "model1.save(filepath='./Trained_Models/Trained_Comb_Model_100BM_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Fx3iXPj0Y_n4",
    "outputId": "e9d49bec-9d9b-4c56-adce-19aea5f35b6d"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "xCNN = Dense(num_classes, activation=\"softmax\")(cnn.output)\n",
    "\n",
    "model2 = Model(inputs=cnn.input, outputs=xCNN)\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model2.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model2.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model2.fit(xtrain, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(xtest, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model2.layers: print(layer.get_config(), layer.get_weights())\n",
    "\n",
    "model2.save(filepath='./Trained_Models/Trained_CNN_Model_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtQ2E4REN6GI",
    "outputId": "f8cbca89-08e3-4a4e-ea19-ac5f3ad16128"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 BM\n",
    "################################################################################################################################################\n",
    "\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model3.fit(traits_BM_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs_traits,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_BM_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "#print(model.layers[3].get_weights()[0])\n",
    "\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_100BM.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qH4j8KaN7Oo",
    "outputId": "e3a7e271-cf46-4e81-c30c-ceaf4f51447c"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 BM, 50 SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model4 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model4.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model4.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model4.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model4.layers: print(layer.get_config(), layer.get_weights())\n",
    "model4.save(filepath='./Trained_Models/Trained_Comb_Model_100BM_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FAJQnqGnbfMw",
    "outputId": "56595f5f-53ce-4d02-8057-b542a6d5c766"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 SNPs\n",
    "################################################################################################################################################\n",
    "xCNN = Dense(num_classes, activation=\"softmax\")(cnn.output)\n",
    "\n",
    "model5 = Model(inputs=cnn.input, outputs=xCNN)\n",
    "\n",
    "model5.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model5.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model5.fit(xtrain, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(xtest, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model5.layers: print(layer.get_config(), layer.get_weights())\n",
    "model5.save(filepath='./Trained_Models/Trained_CNN_Model_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXWI7orOx8zC",
    "outputId": "bcddbd4d-6a68-4a69-92c5-584813aa5198"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 BM, 50 SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "traits_BM50=traits_BM[:,0:50,:]\n",
    "traits_BM_train, traits_BM_test = traits_BM50[int(len(y)*.25):], traits_BM50[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50BM_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjNgaOT_0wnm",
    "outputId": "cc57356e-a594-420b-c4b3-f1687276fcac"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 BM\n",
    "################################################################################################################################################\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()         \n",
    "model3.fit(traits_BM_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_BM_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_50BM.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLWVlN8g82lD",
    "outputId": "c3c68070-c6f3-483f-a16b-c54b05973f0c"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 BM, 20 SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "#X=np.concatenate((u1['simModel1'],u2['simModel2'],u4['simModel4'],u6['simModel6']),axis=0)\n",
    "#X = X[shf]\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50BM_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bEdmwcQucO_e",
    "outputId": "c2e4f49f-7b71-4206-f474-e9a0d9da3edb"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#20 SNPs\n",
    "################################################################################################################################################\n",
    "xCNN = Dense(num_classes, activation=\"softmax\")(cnn.output)\n",
    "\n",
    "model7 = Model(inputs=cnn.input, outputs=xCNN)\n",
    "\n",
    "model7.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model7.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model7.fit(xtrain, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(xtest, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model7.layers: print(layer.get_config(), layer.get_weights())\n",
    "model7.save(filepath='./Trained_Models/Trained_CNN_Model_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEmClYK43qOR",
    "outputId": "e9608731-3cff-4696-bf90-9e11050f09e1"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#10 BM\n",
    "################################################################################################################################################\n",
    "traits_BM10=traits_BM[:,0:10,:]\n",
    "traits_BM_train, traits_BM_test = traits_BM10[int(len(y)*.25):], traits_BM10[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model3.fit(traits_BM_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs_traits,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_BM_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_10BM.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bgPi-7r52xv",
    "outputId": "f4a9b09a-fd6c-4dea-fc89-52562b8ed27c"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#20SNPS, 10 BM\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "traits_BM10=traits_BM[:,0:10,:]\n",
    "traits_BM_train, traits_BM_test = traits_BM10[int(len(y)*.25):], traits_BM10[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10BM_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaUOW_lb4205",
    "outputId": "7e6d4bf0-a71c-44bf-c7b0-dda8e33ccbea"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#1KSNPS, 10 BM\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "#xtest=xtest[:,0:20,:]\n",
    "#xtrain=xtrain[:,0:20,:]\n",
    "traits_BM10=traits_BM[:,0:10,:]\n",
    "traits_BM_train, traits_BM_test = traits_BM10[int(len(y)*.25):], traits_BM10[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10BM_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmdPZdxV699k",
    "outputId": "24794726-b4d5-461c-c2eb-9b72a4ac084c"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#1KSNPS, 50 BM\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM50=traits_BM[:,0:50,:]\n",
    "traits_BM_train, traits_BM_test = traits_BM50[int(len(y)*.25):], traits_BM50[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "cnn = create_cnn(xtest)\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50BM_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJEoto5M8aSf",
    "outputId": "7290e350-be84-4343-aa34-4edd082e4249"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50SNPS, 10 BM\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "traits_BM10=traits_BM[:,0:10,:]\n",
    "traits_BM_train, traits_BM_test = traits_BM10[int(len(y)*.25):], traits_BM10[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10BM_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "140ckPa_9PWp",
    "outputId": "bb768895-b1dd-407e-f900-7dbecb80755b"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#20SNPS, 100 BM\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train, traits_BM_test = traits_BM[int(len(y)*.25):], traits_BM[:int(len(y)*.25)]\n",
    "\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_100BM_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9nDx2HPbIjP",
    "outputId": "52b8a6cf-2974-4111-c935-9939c722adb9"
   },
   "outputs": [],
   "source": [
    "traits_OU = []\n",
    "traits_OU = np.loadtxt(\"./traits/traits_OU.txt\").reshape(30000,-1,100)\n",
    "traits_OU = np.array(traits_OU)\n",
    "\n",
    "scalers = {}\n",
    "for i in range(traits_OU.shape[2]):\n",
    "    scalers[i] = StandardScaler(copy=False)\n",
    "    traits_OU[:, :, i] = scalers[i].fit_transform(traits_OU[:, :, i]) \n",
    "\n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor 1\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "            \n",
    "\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "print (len(X), len(y), len(traits_OU))\n",
    "shf = list(range(len(X)))\n",
    "shuffle(shf)\n",
    "\n",
    "y = y[shf]\n",
    "X = X[shf]\n",
    "traits_OU = traits_OU[shf]\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "ytrain, ytest = y[int(len(y)*.25):], y[:int(len(y)*.25)]\n",
    "traits_OU_train, traits_OU_test = traits_OU[int(len(y)*.25):], traits_OU[:int(len(y)*.25)]\n",
    "\n",
    "\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "                                                                                                                                  # Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "cnn = create_cnn(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJgkHTL9Tddn",
    "outputId": "1484c604-68b9-4104-f544-9d715256a035"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 OU, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model1 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model1.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model1.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model1.layers: print(layer.get_config(), layer.get_weights())\n",
    "model1.save(filepath='./Trained_Models/Trained_Comb_Model_100OU_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xtQ2E4REN6GI",
    "outputId": "4b7f5a1b-0b30-4bcc-a7d0-354788cd57d3"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 OU\n",
    "################################################################################################################################################\n",
    "\n",
    "!mlp = create_mlp(traits_OU_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model3.fit(traits_OU_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs_traits,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_OU_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_100OU.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4qH4j8KaN7Oo",
    "outputId": "85270d63-2aec-4ce6-f9ca-6986834d6771"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 OU, 50 SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model4 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model4.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model4.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model4.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model4.layers: print(layer.get_config(), layer.get_weights())\n",
    "model4.save(filepath='./Trained_Models/Trained_Comb_Model_100OU_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oXWI7orOx8zC",
    "outputId": "aa56ad10-ce27-4d25-fac8-2cc998adc756"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 OU, 50 SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "traits_OU50=traits_OU[:,0:50,:]\n",
    "traits_OU_train, traits_OU_test = traits_OU50[int(len(y)*.25):], traits_OU50[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50OU_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MjNgaOT_0wnm",
    "outputId": "34f570cf-8f13-421a-aa50-b2a6a56a770a"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 OU\n",
    "################################################################################################################################################\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()         \n",
    "model3.fit(traits_OU_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_OU_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_50OU.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jLWVlN8g82lD",
    "outputId": "1ee86ff8-b433-45a6-c757-d4b8662a7e48"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 OU, 20 SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "#X=np.concatenate((u1['simModel1'],u2['simModel2'],u4['simModel4'],u6['simModel6']),axis=0)\n",
    "#X = X[shf]\n",
    "traits_OU50=traits_OU[:,0:50,:]\n",
    "traits_OU_train, traits_OU_test = traits_OU50[int(len(y)*.25):], traits_OU50[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50OU_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XEmClYK43qOR",
    "outputId": "1ba3ee5d-25cc-4482-8973-6868691a9191"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#10 OU\n",
    "################################################################################################################################################\n",
    "traits_OU10=traits_OU[:,0:10,:]\n",
    "traits_OU_train, traits_OU_test = traits_OU10[int(len(y)*.25):], traits_OU10[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model3.fit(traits_OU_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs_traits,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_OU_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_10OU.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9bgPi-7r52xv",
    "outputId": "1858277f-eab4-42ab-e8b2-2a00e91d8d68"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#20SNPS, 10 OU\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "traits_OU10=traits_OU[:,0:10,:]\n",
    "traits_OU_train, traits_OU_test = traits_OU10[int(len(y)*.25):], traits_OU10[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10OU_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vaUOW_lb4205",
    "outputId": "7e4ce685-b2f0-4ef6-9936-68539a49ae8e"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#1KSNPS, 10 OU\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU10=traits_OU[:,0:10,:]\n",
    "traits_OU_train, traits_OU_test = traits_OU10[int(len(y)*.25):], traits_OU10[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10OU_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JmdPZdxV699k",
    "outputId": "28a7e81e-1a3c-4845-9ecf-02c5aceb1154"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#1KSNPS, 50 OU\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU50=traits_OU[:,0:50,:]\n",
    "traits_OU_train, traits_OU_test = traits_OU50[int(len(y)*.25):], traits_OU50[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "cnn = create_cnn(xtest)\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50OU_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FJEoto5M8aSf",
    "outputId": "23d0578d-fd56-4ccb-c35d-06b5887bc51c"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50SNPS, 10 OU\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "traits_OU10=traits_OU[:,0:10,:]\n",
    "traits_OU_train, traits_OU_test = traits_OU10[int(len(y)*.25):], traits_OU10[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10OU_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "140ckPa_9PWp",
    "outputId": "faf1af63-5383-451e-e964-628ccb33a5d9"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#20SNPS, 100 OU\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train, traits_OU_test = traits_OU[int(len(y)*.25):], traits_OU[:int(len(y)*.25)]\n",
    "\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_100OU_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-JkwcGkqahZ",
    "outputId": "08d18c41-9117-4531-eaa2-216a897a84d4"
   },
   "outputs": [],
   "source": [
    "traits_disc = []\n",
    "traits_disc = np.loadtxt(\"./traits/traits_disc.txt\").reshape(30000,-1,100)\n",
    "traits_disc = np.array(traits_disc)\n",
    "\n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor 1\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "            \n",
    "\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "print (len(X), len(y), len(traits_disc))\n",
    "shf = list(range(len(X)))\n",
    "shuffle(shf)\n",
    "\n",
    "y = y[shf]\n",
    "X = X[shf]\n",
    "traits_disc = traits_disc[shf]\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "ytrain, ytest = y[int(len(y)*.25):], y[:int(len(y)*.25)]\n",
    "\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "traits_disc_train, traits_disc_test = traits_disc[int(len(y)*.25):], traits_disc[:int(len(y)*.25)]\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "cnn = create_cnn(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLnlQCPD9RED",
    "outputId": "9904b7d4-0cb0-4037-d951-6cd88de5c381"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 discrete, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model1 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model1.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model1.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model1.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model1.layers: print(layer.get_config(), layer.get_weights())\n",
    "model1.save(filepath='./Trained_Models/Trained_Comb_Model_100disc_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TAaHFwkNC5SF",
    "outputId": "af20f552-c659-4c88-e624-6bff3e623178"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 Discrete\n",
    "################################################################################################################################################\n",
    "traits_disc_train, traits_disc_test = traits_disc[int(len(y)*.25):], traits_disc[:int(len(y)*.25)]\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "opt = SGD(learning_rate=0.001)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model3.fit(traits_disc_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs_traits,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_disc_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_100disc.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZfvCxXABC5SX",
    "outputId": "5508eb70-1fbe-4a27-e25f-3b19d3b3333b"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 discrete, 50 SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model4 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model4.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model4.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model4.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model4.layers: print(layer.get_config(), layer.get_weights())\n",
    "model4.save(filepath='./Trained_Models/Trained_Comb_Model_100disc_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HMz5CqgKC5SY",
    "outputId": "accb9171-f15b-4ab6-9ee8-d2d2ccab8108"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 disc, 50 SNPs\n",
    "################################################################################################################################################\n",
    "traits_disc50=traits_disc[:,0:50,:]\n",
    "traits_disc_train, traits_disc_test = traits_disc50[int(len(y)*.25):], traits_disc50[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50disc_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LVM4GrIVE0ol",
    "outputId": "8676d351-b7c6-4d31-cf38-eb0429ae41a3"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 discrete\n",
    "################################################################################################################################################\n",
    "traits_disc50=traits_disc[:,0:50,:]\n",
    "traits_disc_train, traits_disc_test = traits_disc50[int(len(y)*.25):], traits_disc50[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "          \n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model3.fit(traits_disc_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs_traits,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_disc_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_50disc.acc.mod')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_ugE2A-JE0oz",
    "outputId": "e87f74a4-4ed1-4a84-d202-9398f05e9cf1"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50 discrete, 20 SNPs\n",
    "################################################################################################################################################\n",
    "traits_disc50=traits_disc[:,0:50,:]\n",
    "traits_disc_train, traits_disc_test = traits_disc50[int(len(y)*.25):], traits_disc50[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50disc_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "liXDwXzOE0o0",
    "outputId": "3551ebd9-eb0b-4411-8831-dc32ee284157"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#10 discrete\n",
    "################################################################################################################################################\n",
    "traits_disc10=traits_disc[:,0:10,:]\n",
    "traits_disc_train, traits_disc_test = traits_disc10[int(len(y)*.25):], traits_disc10[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "xMLP = Dense(num_classes, activation=\"softmax\")(mlp.output)\n",
    "\n",
    "model3 = Model(inputs=mlp.input, outputs=xMLP)\n",
    "\n",
    "model3.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model3.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model3.fit(traits_disc_train, ytrain, batch_size=batch_size,\n",
    "          epochs=epochs_traits,\n",
    "          verbose=1,\n",
    "          validation_data=(traits_disc_test, ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model3.layers: print(layer.get_config(), layer.get_weights())\n",
    "model3.save(filepath='./Trained_Models/Trained_Traits_Model_10disc.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0B1h_TqkE0o0",
    "outputId": "e56a2c1e-e86f-470b-9634-e4d3d51918e0"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#20SNPS, 10 discrete\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10disc_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NXtY3u-cE0o0",
    "outputId": "0a66d429-c6e2-41a7-e076-4d59a6dc6bd9"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#1KSNPS, 10 discrete\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "traits_disc10=traits_disc[:,0:10,:]\n",
    "traits_disc_train, traits_disc_test = traits_disc10[int(len(y)*.25):], traits_disc10[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "#xtest=xtest[:,0:20,:]\n",
    "#xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10disc_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cBiwcebIE0o1",
    "outputId": "1667ae85-9e11-4e6b-8816-b09147857126"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#1KSNPS, 50 discrete\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "traits_disc50=traits_disc[:,0:50,:]\n",
    "traits_disc_train, traits_disc_test = traits_disc50[int(len(y)*.25):], traits_disc50[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_50disc_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IacZgKopE0o1",
    "outputId": "78612e46-1906-4e5e-eb7a-2f7a62ee3569"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#50SNPS, 10 discrete\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "traits_disc10=traits_disc[:,0:10,:]\n",
    "traits_disc_train, traits_disc_test = traits_disc10[int(len(y)*.25):], traits_disc10[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:50,:]\n",
    "xtrain=xtrain[:,0:50,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_10disc_50SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-7WAzgnxE0o1",
    "outputId": "9f1ae619-bde9-49d0-c9ef-622e789c9a8f"
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#20SNPS, 100 disc\n",
    "################################################################################################################################################xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train, traits_disc_test = traits_disc[int(len(y)*.25):], traits_disc[:int(len(y)*.25)]\n",
    "\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))\n",
    "\n",
    "# Create the MLP and CNN models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "\n",
    "xtrain, xtest = X[int(len(y)*.25):], X[:int(len(y)*.25)]\n",
    "xtest=xtest[:,0:20,:]\n",
    "xtrain=xtrain[:,0:20,:]\n",
    "cnn = create_cnn(xtest)\n",
    "\n",
    "# Create the input to the final set of layers as the output of both the MLP and CNN\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model6 = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "model6.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model6.summary())\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "start = time.time()\n",
    "model6.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "for layer in model6.layers: print(layer.get_config(), layer.get_weights())\n",
    "model6.save(filepath='./Trained_Models/Trained_Comb_Model_100disc_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJn4Q4eRnWO0"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "model1 = load_model('./Trained_Models/Trained_Traits_Model_100BM.acc.mod')\n",
    "model2 = load_model('./Trained_Models/Trained_Traits_Model_50BM.acc.mod')\n",
    "model3 = load_model('./Trained_Models/Trained_Traits_Model_10BM.acc.mod')\n",
    "model4 = load_model('./Trained_Models/Trained_Traits_Model_100OU.acc.mod')\n",
    "model5 = load_model('./Trained_Models/Trained_Traits_Model_50OU.acc.mod')\n",
    "model6 = load_model('./Trained_Models/Trained_Traits_Model_10OU.acc.mod')\n",
    "model7 = load_model('./Trained_Models/Trained_Traits_Model_100disc.acc.mod')\n",
    "model8 = load_model('./Trained_Models/Trained_Traits_Model_50disc.acc.mod')\n",
    "model9 = load_model('./Trained_Models/Trained_Traits_Model_10disc.acc.mod')\n",
    "model10 = load_model('./Trained_Models/Trained_CNN_Model_1KSNPs.acc.mod')\n",
    "model11 = load_model('./Trained_Models/Trained_CNN_Model_50SNPs.acc.mod')\n",
    "model12 = load_model('./Trained_Models/Trained_CNN_Model_20SNPs.acc.mod')\n",
    "model13 = load_model('./Trained_Models/Trained_Comb_Model_100BM_1KSNPs.acc.mod')\n",
    "model14 = load_model('./Trained_Models/Trained_Comb_Model_50BM_1KSNPs.acc.mod')\n",
    "model15 = load_model('./Trained_Models/Trained_Comb_Model_10BM_1KSNPs.acc.mod')\n",
    "model16 = load_model('./Trained_Models/Trained_Comb_Model_100BM_50SNPs.acc.mod')\n",
    "model17 = load_model('./Trained_Models/Trained_Comb_Model_50BM_50SNPs.acc.mod')\n",
    "model18 = load_model('./Trained_Models/Trained_Comb_Model_10BM_50SNPs.acc.mod')\n",
    "model19 = load_model('./Trained_Models/Trained_Comb_Model_100BM_20SNPs.acc.mod')\n",
    "model20 = load_model('./Trained_Models/Trained_Comb_Model_50BM_20SNPs.acc.mod')\n",
    "model21 = load_model('./Trained_Models/Trained_Comb_Model_10BM_20SNPs.acc.mod')\n",
    "model22 = load_model('./Trained_Models/Trained_Comb_Model_100OU_1KSNPs.acc.mod')\n",
    "model23 = load_model('./Trained_Models/Trained_Comb_Model_50OU_1KSNPs.acc.mod')\n",
    "model24 = load_model('./Trained_Models/Trained_Comb_Model_10OU_1KSNPs.acc.mod')\n",
    "model25 = load_model('./Trained_Models/Trained_Comb_Model_100OU_50SNPs.acc.mod')\n",
    "model26 = load_model('./Trained_Models/Trained_Comb_Model_50OU_50SNPs.acc.mod')\n",
    "model27 = load_model('./Trained_Models/Trained_Comb_Model_10OU_50SNPs.acc.mod')\n",
    "model28 = load_model('./Trained_Models/Trained_Comb_Model_100OU_20SNPs.acc.mod')\n",
    "model29 = load_model('./Trained_Models/Trained_Comb_Model_50OU_20SNPs.acc.mod')\n",
    "model30 = load_model('./Trained_Models/Trained_Comb_Model_10OU_20SNPs.acc.mod')\n",
    "model31 = load_model('./Trained_Models/Trained_Comb_Model_100disc_1KSNPs.acc.mod')\n",
    "model32 = load_model('./Trained_Models/Trained_Comb_Model_50disc_1KSNPs.acc.mod')\n",
    "model33 = load_model('./Trained_Models/Trained_Comb_Model_10disc_1KSNPs.acc.mod')\n",
    "model34 = load_model('./Trained_Models/Trained_Comb_Model_100disc_50SNPs.acc.mod')\n",
    "model35 = load_model('./Trained_Models/Trained_Comb_Model_50disc_50SNPs.acc.mod')\n",
    "model36 = load_model('./Trained_Models/Trained_Comb_Model_10disc_50SNPs.acc.mod')\n",
    "model37 = load_model('./Trained_Models/Trained_Comb_Model_100disc_20SNPs.acc.mod')\n",
    "model38 = load_model('./Trained_Models/Trained_Comb_Model_50disc_20SNPs.acc.mod')\n",
    "model39 = load_model('./Trained_Models/Trained_Comb_Model_10disc_20SNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jF6nrDm1vH0x",
    "outputId": "b29ca1db-ffd1-41af-fe19-f57e77cc2e17"
   },
   "outputs": [],
   "source": [
    "#path = r'./testSims/traits/BM'\n",
    "#all_files = glob.glob(path + \"/*.txt\")\n",
    "#all_files.sort()\n",
    "#all_files.sort(key=os.path.getmtime)\n",
    "#s_all_files=all_files[0:2000]+all_files[3000:4000]+all_files[5000:6000]\n",
    "#s_all_files=all_files[0:2000]+all_files[3000:5000]\n",
    "\n",
    "traits_BM = []\n",
    "#traits_BM = np.stack([np.loadtxt(f) for f in all_files])\n",
    "traits_BM = np.loadtxt(\"./testSims/traits/traits_BM.txt\").reshape(3000,-1,100)\n",
    "traits_BM = np.array(traits_BM)\n",
    "\n",
    "scalers = {}\n",
    "for i in range(traits_BM.shape[2]):\n",
    "    #scalers[i] = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scalers[i] = StandardScaler(copy=False)\n",
    "    traits_BM[:, :, i] = scalers[i].fit_transform(traits_BM[:, :, i]) \n",
    "    \n",
    "\n",
    "traits_BM10=traits_BM[:,0:10,:]\n",
    "traits_BM50=traits_BM[:,0:50,:]\n",
    "\n",
    "#path = r'./testSims/traits/OU'\n",
    "#all_files = glob.glob(path + \"/*.txt\")\n",
    "#all_files.sort()\n",
    "#all_files.sort(key=os.path.getmtime)\n",
    "#s_all_files=all_files[0:2000]+all_files[3000:4000]+all_files[5000:6000]\n",
    "#s_all_files=all_files[0:2000]+all_files[3000:5000]\n",
    "\n",
    "traits_OU = []\n",
    "traits_OU = np.loadtxt(\"./testSims/traits/traits_OU.txt\").reshape(3000,-1,100)\n",
    "traits_OU = np.array(traits_OU)\n",
    "\n",
    "scalers = {}\n",
    "for i in range(traits_OU.shape[2]):\n",
    "    #scalers[i] = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scalers[i] = StandardScaler(copy=False)\n",
    "    traits_OU[:, :, i] = scalers[i].fit_transform(traits_OU[:, :, i]) \n",
    "\n",
    "traits_OU10=traits_OU[:,0:10,:]\n",
    "traits_OU50=traits_OU[:,0:50,:]\n",
    "\n",
    "#path = r'./testSims/traits/discrete'\n",
    "#all_files = glob.glob(path + \"/*.txt\")\n",
    "#all_files.sort()\n",
    "#all_files.sort(key=os.path.getmtime)\n",
    "#s_all_files=all_files[0:2000]+all_files[3000:4000]+all_files[5000:6000]\n",
    "#s_all_files=all_files[0:2000]+all_files[3000:5000]\n",
    "\n",
    "traits_disc = []\n",
    "traits_disc = np.loadtxt(\"./testSims/traits/traits_disc.txt\").reshape(3000,-1,100)\n",
    "traits_disc = np.array(traits_disc)\n",
    "traits_disc10=traits_disc[:,0:10,:]\n",
    "traits_disc50=traits_disc[:,0:50,:]\n",
    "\n",
    "u1 = np.load(\"./testSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./testSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./testSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "xtest=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor 1\n",
    "for arr,array in enumerate(xtest):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            xtest[arr][idx][xtest[arr][idx] == 1] = -1\n",
    "            xtest[arr][idx][xtest[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            xtest[arr][idx][xtest[arr][idx] == 0] = -1\n",
    "            \n",
    "ytest=[0 for i in range(len(u1['Model_1sp']))]\n",
    "ytest.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "ytest.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "ytest = np.array(ytest)\n",
    "\n",
    "#ytest = np_utils.to_categorical(ytest, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "id": "qfi_m7crw2Xo",
    "outputId": "d8eaef70-88e1-432a-f262-30b3f61b096d"
   },
   "outputs": [],
   "source": [
    "#here's the confusion matrix function\n",
    "def makeConfusionMatrixHeatmap(data, title, trueClassOrderLs, predictedClassOrderLs, ax):\n",
    "    data = np.array(data)\n",
    "    data = normalize(data, axis=1, norm='l1')\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    for i in range(len(predictedClassOrderLs)):\n",
    "        for j in reversed(range(len(trueClassOrderLs))):\n",
    "            val = 100*data[j, i]\n",
    "            if val > 50:\n",
    "                c = '0.9'\n",
    "            else:\n",
    "                c = 'black'\n",
    "            ax.text(i + 0.5, j + 0.5, '%.2f%%' % val, horizontalalignment='center', verticalalignment='center', color=c, fontsize=9)\n",
    "\n",
    "    cbar = plt.colorbar(heatmap, cmap=plt.cm.Blues, ax=ax)\n",
    "    cbar.set_label(\"Fraction of simulations assigned to class\", rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.axis('tight')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    #labels\n",
    "    ax.set_xticklabels(predictedClassOrderLs, minor=False, fontsize=9, rotation=45)\n",
    "    ax.set_yticklabels(reversed(trueClassOrderLs), minor=False, fontsize=9)\n",
    "    ax.set_xlabel(\"Predicted class\")\n",
    "    ax.set_ylabel(\"True class\")\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "oVjUXRySZGmu",
    "outputId": "5514c41c-7ab3-4011-f97f-03b9c8e79bcd",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#now the actual work\n",
    "#first get the predictions\n",
    "pred = model1.predict(traits_BM.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "classOrderLs=['one','two','three']\n",
    "\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model2.predict(traits_BM50.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "#get the predictions\n",
    "pred = model3.predict(traits_BM10.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model4.predict(traits_OU.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "classOrderLs=['one','two','three']\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model5.predict(traits_OU50.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model6.predict(traits_OU10.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model7.predict(traits_disc.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "#now the actual work\n",
    "#first get the predictions\n",
    "pred = model8.predict(traits_disc50.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model9.predict(traits_disc10.reshape(3000,-1))\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model10.predict(xtest)\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model11.predict(xtest[:,0:50,:])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model12.predict(xtest[:,0:20,:])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model13.predict([traits_BM.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model14.predict([traits_BM50.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model15.predict([traits_BM10.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model16.predict([traits_BM.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model17.predict([traits_BM50.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model18.predict([traits_BM10.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model19.predict([traits_BM.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model20.predict([traits_BM50.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model21.predict([traits_BM10.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model22.predict([traits_OU.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model23.predict([traits_OU50.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model24.predict([traits_OU10.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model25.predict([traits_OU.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model26.predict([traits_OU50.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model27.predict([traits_OU10.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model28.predict([traits_OU.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model29.predict([traits_OU50.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model30.predict([traits_OU10.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model31.predict([traits_disc.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# get the predictions\n",
    "pred = model32.predict([traits_disc50.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# get the predictions\n",
    "pred = model33.predict([traits_disc10.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model34.predict([traits_disc.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model35.predict([traits_disc50.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model36.predict([traits_disc10.reshape(3000,-1), xtest[:,0:50,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model37.predict([traits_disc.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model38.predict([traits_disc50.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions\n",
    "pred = model39.predict([traits_disc10.reshape(3000,-1), xtest[:,0:20,:]])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
