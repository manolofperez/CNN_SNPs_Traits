{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zgc_VfbydhlW",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle, choice\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from random import shuffle, choice\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "# define a function to build MLP for the trait data.    \n",
    "def create_mlp(traitstrain, regularizer=None):\n",
    "  model = Sequential()\n",
    "  # first layer, remember to remove bias if you are intercalating with batch normalization. ReLu is the activation (nonlinear) function.\n",
    "  model.add(Dense(150, use_bias=False, input_dim=traitstrain.shape[1], activation=\"relu\", kernel_regularizer=regularizers.l1(0.001)))\n",
    "  # batch normalization.\n",
    "  model.add(BatchNormalization())\n",
    "  # second layer.\n",
    "  model.add(Dense(150, use_bias=False, activation=\"relu\", kernel_regularizer=regularizers.l1(0.001)))\n",
    "  model.add(BatchNormalization())\n",
    "  # third layer.\n",
    "  model.add(Dense(50, activation=\"relu\", kernel_regularizer=regularizers.l1(0.001)))\n",
    "  return model\n",
    "\n",
    "# define a function to build a CNN for the SNP data. \n",
    "def create_cnn(xtest, regularizer=None):\n",
    "  # obtain the input dimensions.\n",
    "  inputShape = (xtest.shape[1], xtest.shape[2])\n",
    "  inputs = Input(shape=inputShape)\n",
    "  x = inputs\n",
    "  # first convolutional layer, remember to remove bias if you are intercalating with batch normalization.\n",
    "  x = Conv1D(250, kernel_size=3, activation='relu', use_bias=False, input_shape=(xtest.shape[1], xtest.shape[2]))(x)\n",
    "  # batch normalization.\n",
    "  x = BatchNormalization()(x)\n",
    "  # second layer.\n",
    "  x = Conv1D(250, kernel_size=3, use_bias=False, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  # third layer.\n",
    "  x = Conv1D(250, kernel_size=3, use_bias=False, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  # pool the CNN outputs.\n",
    "  x = MaxPooling1D(pool_size=3)(x)\n",
    "  # flatten in a single vector.\n",
    "  x = Flatten()(x)\n",
    "  # this part is similar to the MLP, a fully connected neural network. We intercalated with dropout to reduce overfitting.\n",
    "  x = Dense(125, activation='relu')(x)\n",
    "  # dropout.\n",
    "  x = Dropout(0.5)(x)\n",
    "  # second layer of the fully connected neural network.\n",
    "  x = Dense(125, activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  # third layer of the fully connected neural network. This one matches the number of nodes coming out of the MLP.\n",
    "  x = Dense(50, kernel_regularizer=regularizer)(x)\n",
    "  x = Activation(\"relu\")(x)\n",
    "  # Construct the CNN\n",
    "  model = Model(inputs, x)\n",
    "  # Return the CNN\n",
    "  return model\n",
    "\n",
    "# define a function to combine the outputs of the MLP and the CNN.\n",
    "# this was obtained from: https://towardsdatascience.com/neural-networks-ensemble-33f33bea7df3\n",
    "class LinearW(Layer):\n",
    "    def __init__(self):\n",
    "        super(LinearW, self).__init__()    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='name',\n",
    "                    shape=(1,1,len(input_shape)),\n",
    "                    initializer='uniform',\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True)\n",
    "    def call(self, inputs):\n",
    "        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n",
    "        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n",
    "        inputs = [tf.expand_dims(i, -1) for i in inputs]\n",
    "        inputs = Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n",
    "        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n",
    "        # weights sum up to one on last dim\n",
    "        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N9nDx2HPbIjP",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "## define variables that will be used to train all networks.\n",
    "# size of the minibatches containing simulations are passed through the network in each epoch.\n",
    "batch_size = 250\n",
    "# number of training iterations (epochs) for the combined networks.\n",
    "epochs = 100\n",
    "# number of scenarios being classified.\n",
    "num_classes = 3\n",
    "\n",
    "# load the traits simulated under the BM model for the 3 scenarios. \n",
    "traits_BM = []\n",
    "traits_BM = np.loadtxt(\"./traits/traits_BM.txt\").reshape(30000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_BM = np.array(traits_BM)\n",
    "\n",
    "# standard scale the continuous (BM) traits\n",
    "scalers_BM = {}\n",
    "for i in range(traits_BM.shape[2]):\n",
    "    scalers_BM[i] = StandardScaler(copy=False)\n",
    "    traits_BM[:, :, i] = scalers_BM[i].fit_transform(traits_BM[:, :, i]) \n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "# transform SNP major alleles in -1 and minor in 1.\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "\n",
    "# create a label vector in the same order as the simulations.\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "# make sure labels, SNP and traits matrices all have the same length.\n",
    "print (len(y), len(X), len(traits_BM))\n",
    "\n",
    "# separate 75% of labels, SNP and traits matrices as training set. The other 25% are assigned to the test set. The two sets are shuffled.\n",
    "ytrain, ytest, xtrain, xtest, traits_BM_train, traits_BM_test  = train_test_split(y,X,traits_BM,test_size=0.25, shuffle=True,stratify=y)\n",
    "\n",
    "# convert labels to a categorical matrix of binary values (0 or 1). The number of rows is the length of the input vector (number of simulations) and the number of columns is the number of classes (3 scenarios).\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "# reshape the traits matrices to input them into the MLP\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJgkHTL9Tddn",
    "outputId": "866e837e-0e7e-4f96-e10b-349e252d23bd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#Combined 100 BM, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the MLP, the CNN and the combined models\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "# using Stochastic Gradient Descent as optimizer and a categorical cross-entropy loss function\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "# save only the epoch with the highest accuracy in the validation set, by using the model checkpoint\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "# fit the model and record running times\n",
    "start = time.time()\n",
    "model.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "# save the model\n",
    "model.save(filepath='./Trained_Models/Trained_Comb_Model_100BM_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9nDx2HPbIjP",
    "outputId": "52b8a6cf-2974-4111-c935-9939c722adb9",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "# Now repeat with traits simulated under the OU model.\n",
    "################################################################################################################################################\n",
    "\n",
    "# load the traits simulated under the OU model for the 3 scenarios. \n",
    "traits_OU = []\n",
    "traits_OU = np.loadtxt(\"./traits/traits_OU.txt\").reshape(30000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_OU = np.array(traits_OU)\n",
    "\n",
    "# standard scale the continuous (OU) traits\n",
    "scalers_OU = {}\n",
    "for i in range(traits_OU.shape[2]):\n",
    "    scalers_OU[i] = StandardScaler(copy=False)\n",
    "    traits_OU[:, :, i] = scalers_OU[i].fit_transform(traits_OU[:, :, i]) \n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor in 1\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "            \n",
    "# create a label vector in the same order as the simulations.\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "# make sure labels, SNP and traits matrices all have the same length.\n",
    "print (len(y), len(X), len(traits_OU))\n",
    "\n",
    "# separate 75% of labels, SNP and traits matrices as training set. The other 25% are assigned to the test set. The two sets are shuffled.\n",
    "ytrain, ytest, xtrain, xtest, traits_OU_train, traits_OU_test  = train_test_split(y,X,traits_OU,test_size=0.25, shuffle=True,stratify=y)\n",
    "\n",
    "# convert labels to a categorical matrix of binary values (0 or 1). The number of rows is the length of the input vector (number of simulations) and the number of columns is the number of classes (3 scenarios).\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "# reshape the traits matrices to input them into the MLP\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))                                                                                                                       # Create the MLP and CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJgkHTL9Tddn",
    "outputId": "1484c604-68b9-4104-f544-9d715256a035",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "# Combined 100 OU, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the MLP, the CNN and the combined models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "# using Stochastic Gradient Descent as optimizer and a categorical cross-entropy loss function\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# save only the epoch with the highest accuracy in the validation set, by using the model checkpoint\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "# fit the model and record running times\n",
    "start = time.time()\n",
    "model.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "# save the model\n",
    "model.save(filepath='./Trained_Models/Trained_Comb_Model_100OU_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-JkwcGkqahZ",
    "outputId": "08d18c41-9117-4531-eaa2-216a897a84d4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "# Now repeat with discrete traits.\n",
    "################################################################################################################################################\n",
    "\n",
    "# load the discrete traits simulated for the 3 scenarios. \n",
    "traits_disc = []\n",
    "traits_disc = np.loadtxt(\"./traits/traits_disc.txt\").reshape(30000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_disc = np.array(traits_disc)\n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor in 1\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "            \n",
    "# create a label vector in the same order as the simulations.\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "# make sure labels, SNP and traits matrices all have the same length.\n",
    "print (len(X), len(y), len(traits_disc))\n",
    "\n",
    "# separate 75% of labels, SNP and traits matrices as training set. The other 25% are assigned to the test set. The two sets are shuffled.\n",
    "ytrain, ytest, xtrain, xtest, traits_disc_train, traits_disc_test  = train_test_split(y,X,traits_disc,test_size=0.25, shuffle=True,stratify=y)\n",
    "\n",
    "# convert labels to a categorical matrix of binary values (0 or 1). The number of rows is the length of the input vector (number of simulations) and the number of columns is the number of classes (3 scenarios).\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "# reshape the traits matrices to input them into the MLP\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLnlQCPD9RED",
    "outputId": "9904b7d4-0cb0-4037-d951-6cd88de5c381",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################################################################################################################################################\n",
    "#100 discrete, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the MLP, the CNN and the combined models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "# using Stochastic Gradient Descent as optimizer and a categorical cross-entropy loss function\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# save only the epoch with the highest accuracy in the validation set, by using the model checkpoint\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "# fit the model and record running times\n",
    "start = time.time()\n",
    "model.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "# save the model\n",
    "model.save(filepath='./Trained_Models/Trained_Comb_Model_100disc_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CJn4Q4eRnWO0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that the models are trained, we will evaluate their accuracy based on the test set. For that, we will build confusion matrices containing the true and predicted scenarions for each simulation on the test set.\n",
    "\n",
    "# first import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# load the trained models.\n",
    "model1 = load_model('./Trained_Models/Trained_Comb_Model_100BM_1KSNPs.acc.mod')\n",
    "model2 = load_model('./Trained_Models/Trained_Comb_Model_100OU_1KSNPs.acc.mod')\n",
    "model3 = load_model('./Trained_Models/Trained_Comb_Model_100disc_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jF6nrDm1vH0x",
    "outputId": "b29ca1db-ffd1-41af-fe19-f57e77cc2e17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the traits simulated under the BM model for the 3 scenarios.\n",
    "traits_BM = []\n",
    "traits_BM = np.loadtxt(\"./testSims/traits/traits_BM.txt\").reshape(3000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_BM = np.array(traits_BM)\n",
    "\n",
    "#Use standard scaling for the continuous (BM) traits.\n",
    "for i in range(traits_BM.shape[2]):\n",
    "    traits_BM[:, :, i] = scalers_BM[i].transform(traits_BM[:, :, i]) \n",
    "\n",
    "# load the traits simulated under the OU model for the 3 scenarios.\n",
    "traits_OU = []\n",
    "traits_OU = np.loadtxt(\"./testSims/traits/traits_OU.txt\").reshape(3000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_OU = np.array(traits_OU)\n",
    "\n",
    "#Use standard scaling for the continuous (OU) traits.\n",
    "for i in range(traits_OU.shape[2]):\n",
    "    traits_OU[:, :, i] = scalers_OU[i].transform(traits_OU[:, :, i]) \n",
    "\n",
    "# load the discrete traits simulated for the 3 scenarios.\n",
    "traits_disc = []\n",
    "traits_disc = np.loadtxt(\"./testSims/traits/traits_disc.txt\").reshape(3000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_disc = np.array(traits_disc)\n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./testSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./testSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./testSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "xtest=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor in 1\n",
    "for arr,array in enumerate(xtest):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            xtest[arr][idx][xtest[arr][idx] == 1] = -1\n",
    "            xtest[arr][idx][xtest[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            xtest[arr][idx][xtest[arr][idx] == 0] = -1\n",
    "\n",
    "# create a label vector in the same order as the simulations.\n",
    "ytest=[0 for i in range(len(u1['Model_1sp']))]\n",
    "ytest.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "ytest.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "ytest = np.array(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "id": "qfi_m7crw2Xo",
    "outputId": "d8eaef70-88e1-432a-f262-30b3f61b096d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define a funtion to build the confusion matrix\n",
    "def makeConfusionMatrixHeatmap(data, title, trueClassOrderLs, predictedClassOrderLs, ax):\n",
    "    data = np.array(data)\n",
    "    data = normalize(data, axis=1, norm='l1')\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    for i in range(len(predictedClassOrderLs)):\n",
    "        for j in reversed(range(len(trueClassOrderLs))):\n",
    "            val = 100*data[j, i]\n",
    "            if val > 50:\n",
    "                c = '0.9'\n",
    "            else:\n",
    "                c = 'black'\n",
    "            ax.text(i + 0.5, j + 0.5, '%.2f%%' % val, horizontalalignment='center', verticalalignment='center', color=c, fontsize=9)\n",
    "\n",
    "    cbar = plt.colorbar(heatmap, cmap=plt.cm.Blues, ax=ax)\n",
    "    cbar.set_label(\"Fraction of simulations assigned to class\", rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.axis('tight')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    #labels\n",
    "    ax.set_xticklabels(predictedClassOrderLs, minor=False, fontsize=9, rotation=45)\n",
    "    ax.set_yticklabels(reversed(trueClassOrderLs), minor=False, fontsize=9)\n",
    "    ax.set_xlabel(\"Predicted class\")\n",
    "    ax.set_ylabel(\"True class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "oVjUXRySZGmu",
    "outputId": "5514c41c-7ab3-4011-f97f-03b9c8e79bcd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now we will plot the confusion matrices for each trained model\n",
    "#first get the predictions\n",
    "pred = model1.predict([traits_BM.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "classOrderLs=['one','two','three']\n",
    "\n",
    "# Print the confusion matrix\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions for the next dataset\n",
    "pred = model2.predict([traits_OU.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions for the next dataset\n",
    "pred = model3.predict([traits_disc.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
