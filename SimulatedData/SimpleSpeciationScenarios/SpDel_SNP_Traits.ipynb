{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Remove this after finishing.\n",
    "#import os\n",
    "#os.chdir('./NewRuns_forGitHub')\n",
    "#print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Zgc_VfbydhlW",
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import all required libraries\n",
    "import sys, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "from random import shuffle, choice\n",
    "import time\n",
    "import os\n",
    "import glob\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import regularizers\n",
    "from random import shuffle, choice\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import sklearn.metrics as metrics\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from sklearn.preprocessing import MinMaxScaler,StandardScaler\n",
    "\n",
    "# define a function to build MLP for the trait data.    \n",
    "def create_mlp(traitstrain, regularizer=None):\n",
    "  model = Sequential()\n",
    "  # first layer, remember to remove bias if you are intercalating with batch normalization. ReLu is the activation (nonlinear) function. We use a L1 regularizer #add more here and to the text.\n",
    "  model.add(Dense(150, use_bias=False, input_dim=traitstrain.shape[1], activation=\"relu\", kernel_regularizer=regularizers.l1(0.001)))\n",
    "  # batch normalization.\n",
    "  model.add(BatchNormalization())\n",
    "  # second layer.\n",
    "  model.add(Dense(150, use_bias=False, activation=\"relu\", kernel_regularizer=regularizers.l1(0.001)))\n",
    "  model.add(BatchNormalization())\n",
    "  # third layer.\n",
    "  model.add(Dense(50, activation=\"relu\", kernel_regularizer=regularizers.l1(0.001)))\n",
    "  return model\n",
    "\n",
    "# define a function to build a CNN for the SNP data. \n",
    "def create_cnn(xtest, regularizer=None):\n",
    "  # obtain the input dimensions.\n",
    "  inputShape = (xtest.shape[1], xtest.shape[2])\n",
    "  inputs = Input(shape=inputShape)\n",
    "  x = inputs\n",
    "  # first convolutional layer, remember to remove bias if you are intercalating with batch normalization.\n",
    "  x = Conv1D(250, kernel_size=3, activation='relu', use_bias=False, input_shape=(xtest.shape[1], xtest.shape[2]))(x)\n",
    "  # batch normalization.\n",
    "  x = BatchNormalization()(x)\n",
    "  # second layer.\n",
    "  x = Conv1D(250, kernel_size=3, use_bias=False, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  # third layer.\n",
    "  x = Conv1D(250, kernel_size=3, use_bias=False, activation='relu')(x)\n",
    "  x = BatchNormalization()(x)\n",
    "  # pool the CNN outputs.\n",
    "  x = MaxPooling1D(pool_size=3)(x)\n",
    "  # flatten in a single vector.\n",
    "  x = Flatten()(x)\n",
    "  # this part is similar to the MLP, a fully connected neural network. We intercalated with dropout to reduce overfitting.\n",
    "  x = Dense(125, activation='relu')(x)\n",
    "  # dropout.\n",
    "  x = Dropout(0.5)(x)\n",
    "  # second layer of the fully connected neural network.\n",
    "  x = Dense(125, activation='relu')(x)\n",
    "  x = Dropout(0.5)(x)\n",
    "  # third layer of the fully connected neural network. This one matches the number of nodes coming out of the MLP.\n",
    "  x = Dense(50, kernel_regularizer=regularizer)(x)\n",
    "  x = Activation(\"relu\")(x)\n",
    "  # Construct the CNN\n",
    "  model = Model(inputs, x)\n",
    "  # Return the CNN\n",
    "  return model\n",
    "\n",
    "# define a function to combine the outputs of the MLP and the CNN.\n",
    "# this was obtained from: https://towardsdatascience.com/neural-networks-ensemble-33f33bea7df3\n",
    "class LinearW(Layer):\n",
    "    def __init__(self):\n",
    "        super(LinearW, self).__init__()    \n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name='name',\n",
    "                    shape=(1,1,len(input_shape)),\n",
    "                    initializer='uniform',\n",
    "                    dtype=tf.float32,\n",
    "                    trainable=True)\n",
    "    def call(self, inputs):\n",
    "        # inputs is a list of tensor of shape [(n_batch, n_feat), ..., (n_batch, n_feat)]\n",
    "        # expand last dim of each input passed [(n_batch, n_feat, 1), ..., (n_batch, n_feat, 1)]\n",
    "        inputs = [tf.expand_dims(i, -1) for i in inputs]\n",
    "        inputs = Concatenate(axis=-1)(inputs) # (n_batch, n_feat, n_inputs)\n",
    "        weights = tf.nn.softmax(self.W, axis=-1) # (1,1,n_inputs)\n",
    "        # weights sum up to one on last dim\n",
    "        return tf.reduce_sum(weights*inputs, axis=-1) # (n_batch, n_feat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "N9nDx2HPbIjP",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000 30000\n"
     ]
    }
   ],
   "source": [
    "## define variables that will be used to train all networks.\n",
    "# size of the minibatches containing simulations are passed through the network in each epoch.\n",
    "batch_size = 250\n",
    "# number of training iterations (epochs) for the SNP only and the combined networks.\n",
    "epochs = 100\n",
    "# number of training iterations (epochs) for the traits only networks.\n",
    "epochs_traits = 500\n",
    "# number of scenarios being classified.\n",
    "num_classes = 3\n",
    "\n",
    "# load the traits simulated under the BM model for the 3 scenarios. \n",
    "traits_BM = []\n",
    "traits_BM = np.loadtxt(\"./traits/traits_BM.txt\").reshape(30000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_BM = np.array(traits_BM)\n",
    "\n",
    "# standard scale the continuous (BM) traits\n",
    "scalers_BM = {}\n",
    "for i in range(traits_BM.shape[2]):\n",
    "    scalers_BM[i] = StandardScaler(copy=False)\n",
    "    traits_BM[:, :, i] = scalers_BM[i].fit_transform(traits_BM[:, :, i]) \n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "# transform SNP major alleles in -1 and minor in 1.\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "\n",
    "# create a label vector in the same order as the simulations.\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "# make sure labels, SNP and traits matrices all have the same length.\n",
    "print (len(y), len(X), len(traits_BM))\n",
    "\n",
    "# separate 75% of labels, SNP and traits matrices as training set. The other 25% are assigned to the test set. The two sets are shuffled.\n",
    "ytrain, ytest, xtrain, xtest, traits_BM_train, traits_BM_test  = train_test_split(y,X,traits_BM,test_size=0.25, shuffle=True,stratify=y)\n",
    "\n",
    "# convert labels to a categorical matrix of binary values (0 or 1). The number of rows is the length of the input vector (number of simulations) and the number of columns is the number of classes (3 scenarios).\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "# reshape the traits matrices to input them into the MLP\n",
    "traits_BM_train=traits_BM_train.reshape((traits_BM_train.shape[0], (traits_BM_train.shape[1]*traits_BM_train.shape[2])))\n",
    "traits_BM_test=traits_BM_test.reshape((traits_BM_test.shape[0], (traits_BM_test.shape[1]*traits_BM_test.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJgkHTL9Tddn",
    "outputId": "866e837e-0e7e-4f96-e10b-349e252d23bd",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_2 (InputLayer)           [(None, 1000, 60)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1d_3 (Conv1D)              (None, 998, 250)     45000       ['input_2[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_7 (BatchNo  (None, 998, 250)    1000        ['conv1d_3[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_4 (Conv1D)              (None, 996, 250)     187500      ['batch_normalization_7[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_8 (BatchNo  (None, 996, 250)    1000        ['conv1d_4[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " conv1d_5 (Conv1D)              (None, 994, 250)     187500      ['batch_normalization_8[0][0]']  \n",
      "                                                                                                  \n",
      " batch_normalization_9 (BatchNo  (None, 994, 250)    1000        ['conv1d_5[0][0]']               \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " max_pooling1d_1 (MaxPooling1D)  (None, 331, 250)    0           ['batch_normalization_9[0][0]']  \n",
      "                                                                                                  \n",
      " flatten_1 (Flatten)            (None, 82750)        0           ['max_pooling1d_1[0][0]']        \n",
      "                                                                                                  \n",
      " dense_8_input (InputLayer)     [(None, 3000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_11 (Dense)               (None, 125)          10343875    ['flatten_1[0][0]']              \n",
      "                                                                                                  \n",
      " dense_8 (Dense)                (None, 150)          450000      ['dense_8_input[0][0]']          \n",
      "                                                                                                  \n",
      " dropout_2 (Dropout)            (None, 125)          0           ['dense_11[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_5 (BatchNo  (None, 150)         600         ['dense_8[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_12 (Dense)               (None, 125)          15750       ['dropout_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_9 (Dense)                (None, 150)          22500       ['batch_normalization_5[0][0]']  \n",
      "                                                                                                  \n",
      " dropout_3 (Dropout)            (None, 125)          0           ['dense_12[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_6 (BatchNo  (None, 150)         600         ['dense_9[0][0]']                \n",
      " rmalization)                                                                                     \n",
      "                                                                                                  \n",
      " dense_13 (Dense)               (None, 50)           6300        ['dropout_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_10 (Dense)               (None, 50)           7550        ['batch_normalization_6[0][0]']  \n",
      "                                                                                                  \n",
      " activation_1 (Activation)      (None, 50)           0           ['dense_13[0][0]']               \n",
      "                                                                                                  \n",
      " linear_w_1 (LinearW)           (None, 50)           2           ['dense_10[0][0]',               \n",
      "                                                                  'activation_1[0][0]']           \n",
      "                                                                                                  \n",
      " dense_14 (Dense)               (None, 50)           2550        ['linear_w_1[0][0]']             \n",
      "                                                                                                  \n",
      " dense_15 (Dense)               (None, 3)            153         ['dense_14[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,272,880\n",
      "Trainable params: 11,270,780\n",
      "Non-trainable params: 2,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 16s 125ms/step - loss: 13.0796 - accuracy: 0.4588 - val_loss: 13.0426 - val_accuracy: 0.5513\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.8332 - accuracy: 0.6148 - val_loss: 12.8264 - val_accuracy: 0.7056\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.6643 - accuracy: 0.6916 - val_loss: 12.5874 - val_accuracy: 0.7781\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.5187 - accuracy: 0.7464 - val_loss: 12.3561 - val_accuracy: 0.8659\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.3725 - accuracy: 0.7979 - val_loss: 12.1661 - val_accuracy: 0.9112\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.2504 - accuracy: 0.8374 - val_loss: 12.0220 - val_accuracy: 0.9457\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.1314 - accuracy: 0.8741 - val_loss: 11.9122 - val_accuracy: 0.9655\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.0357 - accuracy: 0.8972 - val_loss: 11.8305 - val_accuracy: 0.9763\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.9453 - accuracy: 0.9142 - val_loss: 11.7622 - val_accuracy: 0.9811\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.8720 - accuracy: 0.9285 - val_loss: 11.7014 - val_accuracy: 0.9861\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.7958 - accuracy: 0.9414 - val_loss: 11.6450 - val_accuracy: 0.9881\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.7323 - accuracy: 0.9478 - val_loss: 11.5962 - val_accuracy: 0.9893\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.6774 - accuracy: 0.9540 - val_loss: 11.5474 - val_accuracy: 0.9916\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.6211 - accuracy: 0.9611 - val_loss: 11.4987 - val_accuracy: 0.9925\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 10s 116ms/step - loss: 11.5593 - accuracy: 0.9652 - val_loss: 11.4521 - val_accuracy: 0.9944\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.5067 - accuracy: 0.9687 - val_loss: 11.4083 - val_accuracy: 0.9941\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.4602 - accuracy: 0.9713 - val_loss: 11.3650 - val_accuracy: 0.9943\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.4093 - accuracy: 0.9733 - val_loss: 11.3185 - val_accuracy: 0.9965\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.3606 - accuracy: 0.9757 - val_loss: 11.2778 - val_accuracy: 0.9956\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.3155 - accuracy: 0.9776 - val_loss: 11.2343 - val_accuracy: 0.9961\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.2660 - accuracy: 0.9798 - val_loss: 11.1905 - val_accuracy: 0.9975\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.2218 - accuracy: 0.9810 - val_loss: 11.1489 - val_accuracy: 0.9968\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.1761 - accuracy: 0.9820 - val_loss: 11.1063 - val_accuracy: 0.9979\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.1339 - accuracy: 0.9828 - val_loss: 11.0647 - val_accuracy: 0.9976\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.0869 - accuracy: 0.9848 - val_loss: 11.0238 - val_accuracy: 0.9968\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.0427 - accuracy: 0.9856 - val_loss: 10.9813 - val_accuracy: 0.9979\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.9992 - accuracy: 0.9865 - val_loss: 10.9402 - val_accuracy: 0.9977\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 10s 116ms/step - loss: 10.9563 - accuracy: 0.9872 - val_loss: 10.8984 - val_accuracy: 0.9983\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 10s 116ms/step - loss: 10.9156 - accuracy: 0.9864 - val_loss: 10.8573 - val_accuracy: 0.9985\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.8719 - accuracy: 0.9885 - val_loss: 10.8170 - val_accuracy: 0.9983\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.8283 - accuracy: 0.9884 - val_loss: 10.7755 - val_accuracy: 0.9985\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.7873 - accuracy: 0.9881 - val_loss: 10.7347 - val_accuracy: 0.9984\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.7472 - accuracy: 0.9889 - val_loss: 10.6937 - val_accuracy: 0.9989\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.7007 - accuracy: 0.9904 - val_loss: 10.6535 - val_accuracy: 0.9984\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6623 - accuracy: 0.9896 - val_loss: 10.6132 - val_accuracy: 0.9984\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6205 - accuracy: 0.9896 - val_loss: 10.5727 - val_accuracy: 0.9985\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.5782 - accuracy: 0.9915 - val_loss: 10.5321 - val_accuracy: 0.9988\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.5392 - accuracy: 0.9908 - val_loss: 10.4922 - val_accuracy: 0.9989\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4982 - accuracy: 0.9904 - val_loss: 10.4515 - val_accuracy: 0.9995\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4565 - accuracy: 0.9918 - val_loss: 10.4120 - val_accuracy: 0.9989\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4154 - accuracy: 0.9918 - val_loss: 10.3721 - val_accuracy: 0.9992\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.3736 - accuracy: 0.9929 - val_loss: 10.3323 - val_accuracy: 0.9988\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.3340 - accuracy: 0.9934 - val_loss: 10.2924 - val_accuracy: 0.9993\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2939 - accuracy: 0.9928 - val_loss: 10.2528 - val_accuracy: 0.9992\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2526 - accuracy: 0.9940 - val_loss: 10.2134 - val_accuracy: 0.9992\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2119 - accuracy: 0.9939 - val_loss: 10.1745 - val_accuracy: 0.9992\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.1743 - accuracy: 0.9942 - val_loss: 10.1351 - val_accuracy: 0.9991\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.1333 - accuracy: 0.9941 - val_loss: 10.0952 - val_accuracy: 0.9995\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 10s 114ms/step - loss: 10.0942 - accuracy: 0.9940 - val_loss: 10.0568 - val_accuracy: 0.9989\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0547 - accuracy: 0.9943 - val_loss: 10.0173 - val_accuracy: 0.9992\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0138 - accuracy: 0.9948 - val_loss: 9.9789 - val_accuracy: 0.9991\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.9766 - accuracy: 0.9946 - val_loss: 9.9399 - val_accuracy: 0.9991\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.9383 - accuracy: 0.9941 - val_loss: 9.9010 - val_accuracy: 0.9992\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8970 - accuracy: 0.9952 - val_loss: 9.8622 - val_accuracy: 0.9993\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8585 - accuracy: 0.9953 - val_loss: 9.8231 - val_accuracy: 0.9995\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8200 - accuracy: 0.9951 - val_loss: 9.7851 - val_accuracy: 0.9992\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.7824 - accuracy: 0.9940 - val_loss: 9.7469 - val_accuracy: 0.9992\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.7414 - accuracy: 0.9954 - val_loss: 9.7079 - val_accuracy: 0.9995\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.7025 - accuracy: 0.9956 - val_loss: 9.6699 - val_accuracy: 0.9993\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6636 - accuracy: 0.9961 - val_loss: 9.6318 - val_accuracy: 0.9993\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6260 - accuracy: 0.9957 - val_loss: 9.5937 - val_accuracy: 0.9993\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5868 - accuracy: 0.9961 - val_loss: 9.5558 - val_accuracy: 0.9992\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5483 - accuracy: 0.9960 - val_loss: 9.5181 - val_accuracy: 0.9993\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5083 - accuracy: 0.9967 - val_loss: 9.4798 - val_accuracy: 0.9995\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.4734 - accuracy: 0.9959 - val_loss: 9.4424 - val_accuracy: 0.9995\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.4364 - accuracy: 0.9956 - val_loss: 9.4049 - val_accuracy: 0.9992\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3983 - accuracy: 0.9953 - val_loss: 9.3669 - val_accuracy: 0.9995\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3580 - accuracy: 0.9969 - val_loss: 9.3296 - val_accuracy: 0.9995\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3223 - accuracy: 0.9963 - val_loss: 9.2921 - val_accuracy: 0.9995\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.2852 - accuracy: 0.9963 - val_loss: 9.2552 - val_accuracy: 0.9993\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 10s 114ms/step - loss: 9.2456 - accuracy: 0.9969 - val_loss: 9.2177 - val_accuracy: 0.9995\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.2096 - accuracy: 0.9968 - val_loss: 9.1805 - val_accuracy: 0.9995\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.1717 - accuracy: 0.9971 - val_loss: 9.1438 - val_accuracy: 0.9995\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.1357 - accuracy: 0.9960 - val_loss: 9.1065 - val_accuracy: 0.9995\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0984 - accuracy: 0.9966 - val_loss: 9.0699 - val_accuracy: 0.9993\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0607 - accuracy: 0.9968 - val_loss: 9.0330 - val_accuracy: 0.9995\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0228 - accuracy: 0.9972 - val_loss: 8.9963 - val_accuracy: 0.9995\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9899 - accuracy: 0.9964 - val_loss: 8.9594 - val_accuracy: 0.9996\n",
      "Epoch 79/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9496 - accuracy: 0.9970 - val_loss: 8.9228 - val_accuracy: 0.9995\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9122 - accuracy: 0.9975 - val_loss: 8.8866 - val_accuracy: 0.9995\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.8765 - accuracy: 0.9975 - val_loss: 8.8503 - val_accuracy: 0.9995\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.8402 - accuracy: 0.9971 - val_loss: 8.8142 - val_accuracy: 0.9995\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.8053 - accuracy: 0.9968 - val_loss: 8.7773 - val_accuracy: 0.9996\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7679 - accuracy: 0.9973 - val_loss: 8.7412 - val_accuracy: 0.9996\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7312 - accuracy: 0.9972 - val_loss: 8.7055 - val_accuracy: 0.9995\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6964 - accuracy: 0.9972 - val_loss: 8.6696 - val_accuracy: 0.9995\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6600 - accuracy: 0.9973 - val_loss: 8.6338 - val_accuracy: 0.9995\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6229 - accuracy: 0.9974 - val_loss: 8.5981 - val_accuracy: 0.9995\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5885 - accuracy: 0.9974 - val_loss: 8.5622 - val_accuracy: 0.9995\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5518 - accuracy: 0.9974 - val_loss: 8.5266 - val_accuracy: 0.9995\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5163 - accuracy: 0.9978 - val_loss: 8.4916 - val_accuracy: 0.9993\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4810 - accuracy: 0.9973 - val_loss: 8.4558 - val_accuracy: 0.9995\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4460 - accuracy: 0.9972 - val_loss: 8.4205 - val_accuracy: 0.9995\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4100 - accuracy: 0.9974 - val_loss: 8.3852 - val_accuracy: 0.9995\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3735 - accuracy: 0.9981 - val_loss: 8.3501 - val_accuracy: 0.9995\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3391 - accuracy: 0.9978 - val_loss: 8.3151 - val_accuracy: 0.9995\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3036 - accuracy: 0.9980 - val_loss: 8.2800 - val_accuracy: 0.9995\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2694 - accuracy: 0.9977 - val_loss: 8.2453 - val_accuracy: 0.9995\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2330 - accuracy: 0.9980 - val_loss: 8.2101 - val_accuracy: 0.9995\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.1999 - accuracy: 0.9976 - val_loss: 8.1753 - val_accuracy: 0.9995\n",
      "Time: 1040.144615650177\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-01 13:47:19.069197: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./Trained_Models/Trained_Comb_Model_100BM_1KSNPs.acc.mod/assets\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################################################\n",
    "#Combined 100 BM, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the MLP, the CNN and the combined models\n",
    "mlp = create_mlp(traits_BM_train)\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "# using Stochastic Gradient Descent as optimizer and a categorical cross-entropy loss function\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "# save only the epoch with the highest accuracy in the validation set, by using the model checkpoint\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "# fit the model and record running times\n",
    "start = time.time()\n",
    "model.fit([traits_BM_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_BM_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "# save the model\n",
    "model.save(filepath='./Trained_Models/Trained_Comb_Model_100BM_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N9nDx2HPbIjP",
    "outputId": "52b8a6cf-2974-4111-c935-9939c722adb9",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000 30000\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################################################\n",
    "# Now repeat with traits simulated under the OU model.\n",
    "################################################################################################################################################\n",
    "\n",
    "# load the traits simulated under the OU model for the 3 scenarios. \n",
    "traits_OU = []\n",
    "traits_OU = np.loadtxt(\"./traits/traits_OU.txt\").reshape(30000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_OU = np.array(traits_OU)\n",
    "\n",
    "# standard scale the continuous (OU) traits\n",
    "scalers_OU = {}\n",
    "for i in range(traits_OU.shape[2]):\n",
    "    scalers_OU[i] = StandardScaler(copy=False)\n",
    "    traits_OU[:, :, i] = scalers_OU[i].fit_transform(traits_OU[:, :, i]) \n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor in 1\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "            \n",
    "# create a label vector in the same order as the simulations.\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "# make sure labels, SNP and traits matrices all have the same length.\n",
    "print (len(y), len(X), len(traits_OU))\n",
    "\n",
    "# separate 75% of labels, SNP and traits matrices as training set. The other 25% are assigned to the test set. The two sets are shuffled.\n",
    "ytrain, ytest, xtrain, xtest, traits_OU_train, traits_OU_test  = train_test_split(y,X,traits_OU,test_size=0.25, shuffle=True,stratify=y)\n",
    "\n",
    "# convert labels to a categorical matrix of binary values (0 or 1). The number of rows is the length of the input vector (number of simulations) and the number of columns is the number of classes (3 scenarios).\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "# reshape the traits matrices to input them into the MLP\n",
    "traits_OU_train=traits_OU_train.reshape((traits_OU_train.shape[0], (traits_OU_train.shape[1]*traits_OU_train.shape[2])))\n",
    "traits_OU_test=traits_OU_test.reshape((traits_OU_test.shape[0], (traits_OU_test.shape[1]*traits_OU_test.shape[2])))                                                                                                                       # Create the MLP and CNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NJgkHTL9Tddn",
    "outputId": "1484c604-68b9-4104-f544-9d715256a035",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_3 (InputLayer)           [(None, 1000, 60)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1d_6 (Conv1D)              (None, 998, 250)     45000       ['input_3[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_12 (BatchN  (None, 998, 250)    1000        ['conv1d_6[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_7 (Conv1D)              (None, 996, 250)     187500      ['batch_normalization_12[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_13 (BatchN  (None, 996, 250)    1000        ['conv1d_7[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_8 (Conv1D)              (None, 994, 250)     187500      ['batch_normalization_13[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_14 (BatchN  (None, 994, 250)    1000        ['conv1d_8[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling1d_2 (MaxPooling1D)  (None, 331, 250)    0           ['batch_normalization_14[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_2 (Flatten)            (None, 82750)        0           ['max_pooling1d_2[0][0]']        \n",
      "                                                                                                  \n",
      " dense_16_input (InputLayer)    [(None, 3000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_19 (Dense)               (None, 125)          10343875    ['flatten_2[0][0]']              \n",
      "                                                                                                  \n",
      " dense_16 (Dense)               (None, 150)          450000      ['dense_16_input[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_4 (Dropout)            (None, 125)          0           ['dense_19[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_10 (BatchN  (None, 150)         600         ['dense_16[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_20 (Dense)               (None, 125)          15750       ['dropout_4[0][0]']              \n",
      "                                                                                                  \n",
      " dense_17 (Dense)               (None, 150)          22500       ['batch_normalization_10[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_5 (Dropout)            (None, 125)          0           ['dense_20[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_11 (BatchN  (None, 150)         600         ['dense_17[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_21 (Dense)               (None, 50)           6300        ['dropout_5[0][0]']              \n",
      "                                                                                                  \n",
      " dense_18 (Dense)               (None, 50)           7550        ['batch_normalization_11[0][0]'] \n",
      "                                                                                                  \n",
      " activation_2 (Activation)      (None, 50)           0           ['dense_21[0][0]']               \n",
      "                                                                                                  \n",
      " linear_w_2 (LinearW)           (None, 50)           2           ['dense_18[0][0]',               \n",
      "                                                                  'activation_2[0][0]']           \n",
      "                                                                                                  \n",
      " dense_22 (Dense)               (None, 50)           2550        ['linear_w_2[0][0]']             \n",
      "                                                                                                  \n",
      " dense_23 (Dense)               (None, 3)            153         ['dense_22[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,272,880\n",
      "Trainable params: 11,270,780\n",
      "Non-trainable params: 2,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 12s 124ms/step - loss: 13.3536 - accuracy: 0.3523 - val_loss: 13.1784 - val_accuracy: 0.3428\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 13.0116 - accuracy: 0.4916 - val_loss: 12.9465 - val_accuracy: 0.5781\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.7825 - accuracy: 0.5971 - val_loss: 12.6852 - val_accuracy: 0.7243\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.6313 - accuracy: 0.6675 - val_loss: 12.4772 - val_accuracy: 0.8001\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.4926 - accuracy: 0.7240 - val_loss: 12.2852 - val_accuracy: 0.8747\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.3568 - accuracy: 0.7738 - val_loss: 12.1397 - val_accuracy: 0.9137\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.2366 - accuracy: 0.8101 - val_loss: 12.0055 - val_accuracy: 0.9375\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.1203 - accuracy: 0.8504 - val_loss: 11.8970 - val_accuracy: 0.9607\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.0164 - accuracy: 0.8796 - val_loss: 11.8058 - val_accuracy: 0.9735\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.9224 - accuracy: 0.9048 - val_loss: 11.7309 - val_accuracy: 0.9801\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.8422 - accuracy: 0.9187 - val_loss: 11.6635 - val_accuracy: 0.9847\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.7650 - accuracy: 0.9346 - val_loss: 11.6060 - val_accuracy: 0.9865\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.6976 - accuracy: 0.9450 - val_loss: 11.5507 - val_accuracy: 0.9904\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.6351 - accuracy: 0.9514 - val_loss: 11.5007 - val_accuracy: 0.9916\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.5750 - accuracy: 0.9592 - val_loss: 11.4526 - val_accuracy: 0.9931\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.5221 - accuracy: 0.9605 - val_loss: 11.4051 - val_accuracy: 0.9945\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.4665 - accuracy: 0.9676 - val_loss: 11.3577 - val_accuracy: 0.9963\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.4133 - accuracy: 0.9698 - val_loss: 11.3146 - val_accuracy: 0.9956\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.3643 - accuracy: 0.9726 - val_loss: 11.2717 - val_accuracy: 0.9964\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.3186 - accuracy: 0.9755 - val_loss: 11.2272 - val_accuracy: 0.9968\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.2687 - accuracy: 0.9779 - val_loss: 11.1850 - val_accuracy: 0.9968\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.2229 - accuracy: 0.9785 - val_loss: 11.1416 - val_accuracy: 0.9979\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.1734 - accuracy: 0.9815 - val_loss: 11.1007 - val_accuracy: 0.9972\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.1288 - accuracy: 0.9823 - val_loss: 11.0560 - val_accuracy: 0.9983\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.0848 - accuracy: 0.9837 - val_loss: 11.0157 - val_accuracy: 0.9973\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.0388 - accuracy: 0.9846 - val_loss: 10.9739 - val_accuracy: 0.9977\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.9967 - accuracy: 0.9858 - val_loss: 10.9323 - val_accuracy: 0.9981\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.9533 - accuracy: 0.9860 - val_loss: 10.8913 - val_accuracy: 0.9979\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 10s 116ms/step - loss: 10.9092 - accuracy: 0.9862 - val_loss: 10.8498 - val_accuracy: 0.9985\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.8656 - accuracy: 0.9874 - val_loss: 10.8085 - val_accuracy: 0.9987\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.8273 - accuracy: 0.9871 - val_loss: 10.7679 - val_accuracy: 0.9987\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.7814 - accuracy: 0.9893 - val_loss: 10.7266 - val_accuracy: 0.9988\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.7385 - accuracy: 0.9897 - val_loss: 10.6862 - val_accuracy: 0.9987\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6952 - accuracy: 0.9907 - val_loss: 10.6452 - val_accuracy: 0.9989\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6540 - accuracy: 0.9900 - val_loss: 10.6062 - val_accuracy: 0.9984\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6151 - accuracy: 0.9903 - val_loss: 10.5649 - val_accuracy: 0.9988\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.5729 - accuracy: 0.9909 - val_loss: 10.5247 - val_accuracy: 0.9988\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.5310 - accuracy: 0.9919 - val_loss: 10.4841 - val_accuracy: 0.9993\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4897 - accuracy: 0.9922 - val_loss: 10.4444 - val_accuracy: 0.9989\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4498 - accuracy: 0.9922 - val_loss: 10.4043 - val_accuracy: 0.9995\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4078 - accuracy: 0.9924 - val_loss: 10.3652 - val_accuracy: 0.9987\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.3677 - accuracy: 0.9932 - val_loss: 10.3247 - val_accuracy: 0.9992\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.3260 - accuracy: 0.9934 - val_loss: 10.2854 - val_accuracy: 0.9988\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2852 - accuracy: 0.9936 - val_loss: 10.2463 - val_accuracy: 0.9988\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2472 - accuracy: 0.9934 - val_loss: 10.2062 - val_accuracy: 0.9995\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2059 - accuracy: 0.9940 - val_loss: 10.1662 - val_accuracy: 0.9996\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.1682 - accuracy: 0.9932 - val_loss: 10.1284 - val_accuracy: 0.9988\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.1275 - accuracy: 0.9935 - val_loss: 10.0892 - val_accuracy: 0.9987\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0869 - accuracy: 0.9940 - val_loss: 10.0496 - val_accuracy: 0.9993\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0494 - accuracy: 0.9940 - val_loss: 10.0102 - val_accuracy: 0.9996\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0097 - accuracy: 0.9935 - val_loss: 9.9717 - val_accuracy: 0.9989\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.9694 - accuracy: 0.9948 - val_loss: 9.9322 - val_accuracy: 0.9996\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.9289 - accuracy: 0.9947 - val_loss: 9.8934 - val_accuracy: 0.9996\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8922 - accuracy: 0.9946 - val_loss: 9.8543 - val_accuracy: 0.9997\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8521 - accuracy: 0.9949 - val_loss: 9.8161 - val_accuracy: 0.9996\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8110 - accuracy: 0.9956 - val_loss: 9.7782 - val_accuracy: 0.9989\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.7731 - accuracy: 0.9953 - val_loss: 9.7395 - val_accuracy: 0.9996\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.7356 - accuracy: 0.9949 - val_loss: 9.7009 - val_accuracy: 0.9996\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6953 - accuracy: 0.9960 - val_loss: 9.6627 - val_accuracy: 0.9995\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6580 - accuracy: 0.9955 - val_loss: 9.6245 - val_accuracy: 0.9995\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6189 - accuracy: 0.9958 - val_loss: 9.5859 - val_accuracy: 0.9996\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5796 - accuracy: 0.9964 - val_loss: 9.5484 - val_accuracy: 0.9996\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5437 - accuracy: 0.9956 - val_loss: 9.5102 - val_accuracy: 0.9996\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5041 - accuracy: 0.9958 - val_loss: 9.4725 - val_accuracy: 0.9995\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.4665 - accuracy: 0.9963 - val_loss: 9.4347 - val_accuracy: 0.9996\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.4279 - accuracy: 0.9964 - val_loss: 9.3972 - val_accuracy: 0.9996\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3906 - accuracy: 0.9960 - val_loss: 9.3594 - val_accuracy: 0.9995\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3525 - accuracy: 0.9960 - val_loss: 9.3218 - val_accuracy: 0.9996\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3152 - accuracy: 0.9959 - val_loss: 9.2846 - val_accuracy: 0.9996\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.2771 - accuracy: 0.9964 - val_loss: 9.2476 - val_accuracy: 0.9996\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.2395 - accuracy: 0.9967 - val_loss: 9.2100 - val_accuracy: 0.9996\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.2043 - accuracy: 0.9961 - val_loss: 9.1731 - val_accuracy: 0.9996\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.1654 - accuracy: 0.9965 - val_loss: 9.1359 - val_accuracy: 0.9996\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.1278 - accuracy: 0.9968 - val_loss: 9.0988 - val_accuracy: 0.9996\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0904 - accuracy: 0.9968 - val_loss: 9.0620 - val_accuracy: 0.9995\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0544 - accuracy: 0.9968 - val_loss: 9.0252 - val_accuracy: 0.9995\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0171 - accuracy: 0.9971 - val_loss: 8.9889 - val_accuracy: 0.9995\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9803 - accuracy: 0.9966 - val_loss: 8.9518 - val_accuracy: 0.9996\n",
      "Epoch 79/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9440 - accuracy: 0.9967 - val_loss: 8.9160 - val_accuracy: 0.9995\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9074 - accuracy: 0.9968 - val_loss: 8.8788 - val_accuracy: 0.9996\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.8690 - accuracy: 0.9978 - val_loss: 8.8427 - val_accuracy: 0.9995\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.8327 - accuracy: 0.9977 - val_loss: 8.8063 - val_accuracy: 0.9995\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7970 - accuracy: 0.9971 - val_loss: 8.7697 - val_accuracy: 0.9997\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7615 - accuracy: 0.9967 - val_loss: 8.7337 - val_accuracy: 0.9997\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7258 - accuracy: 0.9968 - val_loss: 8.6977 - val_accuracy: 0.9996\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6891 - accuracy: 0.9977 - val_loss: 8.6619 - val_accuracy: 0.9996\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6515 - accuracy: 0.9976 - val_loss: 8.6259 - val_accuracy: 0.9997\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6170 - accuracy: 0.9974 - val_loss: 8.5904 - val_accuracy: 0.9996\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5821 - accuracy: 0.9974 - val_loss: 8.5550 - val_accuracy: 0.9996\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5456 - accuracy: 0.9976 - val_loss: 8.5190 - val_accuracy: 0.9996\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5092 - accuracy: 0.9978 - val_loss: 8.4836 - val_accuracy: 0.9995\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4729 - accuracy: 0.9980 - val_loss: 8.4484 - val_accuracy: 0.9995\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4380 - accuracy: 0.9979 - val_loss: 8.4128 - val_accuracy: 0.9996\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4027 - accuracy: 0.9976 - val_loss: 8.3774 - val_accuracy: 0.9997\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3671 - accuracy: 0.9976 - val_loss: 8.3426 - val_accuracy: 0.9995\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3324 - accuracy: 0.9976 - val_loss: 8.3074 - val_accuracy: 0.9996\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2966 - accuracy: 0.9980 - val_loss: 8.2722 - val_accuracy: 0.9996\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2624 - accuracy: 0.9976 - val_loss: 8.2372 - val_accuracy: 0.9997\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2278 - accuracy: 0.9979 - val_loss: 8.2028 - val_accuracy: 0.9996\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.1940 - accuracy: 0.9970 - val_loss: 8.1678 - val_accuracy: 0.9996\n",
      "Time: 1036.2018325328827\n",
      "INFO:tensorflow:Assets written to: ./Trained_Models/Trained_Comb_Model_100OU_1KSNPs.acc.mod/assets\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################################################\n",
    "# Combined 100 OU, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the MLP, the CNN and the combined models\n",
    "mlp = create_mlp(traits_OU_train)\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "# using Stochastic Gradient Descent as optimizer and a categorical cross-entropy loss function\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# save only the epoch with the highest accuracy in the validation set, by using the model checkpoint\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "# fit the model and record running times\n",
    "start = time.time()\n",
    "model.fit([traits_OU_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_OU_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "# save the model\n",
    "model.save(filepath='./Trained_Models/Trained_Comb_Model_100OU_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Y-JkwcGkqahZ",
    "outputId": "08d18c41-9117-4531-eaa2-216a897a84d4",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000 30000 30000\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################################################\n",
    "# Now repeat with discrete traits.\n",
    "################################################################################################################################################\n",
    "\n",
    "# load the discrete traits simulated for the 3 scenarios. \n",
    "traits_disc = []\n",
    "traits_disc = np.loadtxt(\"./traits/traits_disc.txt\").reshape(30000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_disc = np.array(traits_disc)\n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./trainingSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./trainingSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./trainingSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "X=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor in 1\n",
    "for arr,array in enumerate(X):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            X[arr][idx][X[arr][idx] == 1] = -1\n",
    "            X[arr][idx][X[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            X[arr][idx][X[arr][idx] == 0] = -1\n",
    "            \n",
    "# create a label vector in the same order as the simulations.\n",
    "y=[0 for i in range(len(u1['Model_1sp']))]\n",
    "y.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "y.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "y = np.array(y)\n",
    "\n",
    "# make sure labels, SNP and traits matrices all have the same length.\n",
    "print (len(X), len(y), len(traits_disc))\n",
    "\n",
    "# separate 75% of labels, SNP and traits matrices as training set. The other 25% are assigned to the test set. The two sets are shuffled.\n",
    "ytrain, ytest, xtrain, xtest, traits_disc_train, traits_disc_test  = train_test_split(y,X,traits_disc,test_size=0.25, shuffle=True,stratify=y)\n",
    "\n",
    "# convert labels to a categorical matrix of binary values (0 or 1). The number of rows is the length of the input vector (number of simulations) and the number of columns is the number of classes (3 scenarios).\n",
    "ytest = np_utils.to_categorical(ytest, num_classes)\n",
    "ytrain = np_utils.to_categorical(ytrain, num_classes)\n",
    "# reshape the traits matrices to input them into the MLP\n",
    "traits_disc_train=traits_disc_train.reshape((traits_disc_train.shape[0], (traits_disc_train.shape[1]*traits_disc_train.shape[2])))\n",
    "traits_disc_test=traits_disc_test.reshape((traits_disc_test.shape[0], (traits_disc_test.shape[1]*traits_disc_test.shape[2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dLnlQCPD9RED",
    "outputId": "9904b7d4-0cb0-4037-d951-6cd88de5c381",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_7\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_4 (InputLayer)           [(None, 1000, 60)]   0           []                               \n",
      "                                                                                                  \n",
      " conv1d_9 (Conv1D)              (None, 998, 250)     45000       ['input_4[0][0]']                \n",
      "                                                                                                  \n",
      " batch_normalization_17 (BatchN  (None, 998, 250)    1000        ['conv1d_9[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_10 (Conv1D)             (None, 996, 250)     187500      ['batch_normalization_17[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_18 (BatchN  (None, 996, 250)    1000        ['conv1d_10[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " conv1d_11 (Conv1D)             (None, 994, 250)     187500      ['batch_normalization_18[0][0]'] \n",
      "                                                                                                  \n",
      " batch_normalization_19 (BatchN  (None, 994, 250)    1000        ['conv1d_11[0][0]']              \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " max_pooling1d_3 (MaxPooling1D)  (None, 331, 250)    0           ['batch_normalization_19[0][0]'] \n",
      "                                                                                                  \n",
      " flatten_3 (Flatten)            (None, 82750)        0           ['max_pooling1d_3[0][0]']        \n",
      "                                                                                                  \n",
      " dense_24_input (InputLayer)    [(None, 3000)]       0           []                               \n",
      "                                                                                                  \n",
      " dense_27 (Dense)               (None, 125)          10343875    ['flatten_3[0][0]']              \n",
      "                                                                                                  \n",
      " dense_24 (Dense)               (None, 150)          450000      ['dense_24_input[0][0]']         \n",
      "                                                                                                  \n",
      " dropout_6 (Dropout)            (None, 125)          0           ['dense_27[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_15 (BatchN  (None, 150)         600         ['dense_24[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_28 (Dense)               (None, 125)          15750       ['dropout_6[0][0]']              \n",
      "                                                                                                  \n",
      " dense_25 (Dense)               (None, 150)          22500       ['batch_normalization_15[0][0]'] \n",
      "                                                                                                  \n",
      " dropout_7 (Dropout)            (None, 125)          0           ['dense_28[0][0]']               \n",
      "                                                                                                  \n",
      " batch_normalization_16 (BatchN  (None, 150)         600         ['dense_25[0][0]']               \n",
      " ormalization)                                                                                    \n",
      "                                                                                                  \n",
      " dense_29 (Dense)               (None, 50)           6300        ['dropout_7[0][0]']              \n",
      "                                                                                                  \n",
      " dense_26 (Dense)               (None, 50)           7550        ['batch_normalization_16[0][0]'] \n",
      "                                                                                                  \n",
      " activation_3 (Activation)      (None, 50)           0           ['dense_29[0][0]']               \n",
      "                                                                                                  \n",
      " linear_w_3 (LinearW)           (None, 50)           2           ['dense_26[0][0]',               \n",
      "                                                                  'activation_3[0][0]']           \n",
      "                                                                                                  \n",
      " dense_30 (Dense)               (None, 50)           2550        ['linear_w_3[0][0]']             \n",
      "                                                                                                  \n",
      " dense_31 (Dense)               (None, 3)            153         ['dense_30[0][0]']               \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 11,272,880\n",
      "Trainable params: 11,270,780\n",
      "Non-trainable params: 2,100\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "Epoch 1/100\n",
      "90/90 [==============================] - 12s 124ms/step - loss: 13.2011 - accuracy: 0.3715 - val_loss: 13.1275 - val_accuracy: 0.3569\n",
      "Epoch 2/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 13.0269 - accuracy: 0.4671 - val_loss: 12.9941 - val_accuracy: 0.5071\n",
      "Epoch 3/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.8430 - accuracy: 0.5817 - val_loss: 12.7402 - val_accuracy: 0.7244\n",
      "Epoch 4/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.6685 - accuracy: 0.6609 - val_loss: 12.4822 - val_accuracy: 0.8247\n",
      "Epoch 5/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.5049 - accuracy: 0.7271 - val_loss: 12.2680 - val_accuracy: 0.8968\n",
      "Epoch 6/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.3605 - accuracy: 0.7798 - val_loss: 12.0990 - val_accuracy: 0.9311\n",
      "Epoch 7/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.2355 - accuracy: 0.8203 - val_loss: 11.9843 - val_accuracy: 0.9513\n",
      "Epoch 8/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.1179 - accuracy: 0.8524 - val_loss: 11.8764 - val_accuracy: 0.9677\n",
      "Epoch 9/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 12.0103 - accuracy: 0.8860 - val_loss: 11.7912 - val_accuracy: 0.9787\n",
      "Epoch 10/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.9232 - accuracy: 0.9041 - val_loss: 11.7149 - val_accuracy: 0.9841\n",
      "Epoch 11/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.8346 - accuracy: 0.9225 - val_loss: 11.6519 - val_accuracy: 0.9875\n",
      "Epoch 12/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.7612 - accuracy: 0.9356 - val_loss: 11.5948 - val_accuracy: 0.9912\n",
      "Epoch 13/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.6922 - accuracy: 0.9460 - val_loss: 11.5404 - val_accuracy: 0.9933\n",
      "Epoch 14/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.6291 - accuracy: 0.9528 - val_loss: 11.4906 - val_accuracy: 0.9952\n",
      "Epoch 15/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.5727 - accuracy: 0.9586 - val_loss: 11.4435 - val_accuracy: 0.9952\n",
      "Epoch 16/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.5179 - accuracy: 0.9627 - val_loss: 11.3980 - val_accuracy: 0.9956\n",
      "Epoch 17/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.4618 - accuracy: 0.9673 - val_loss: 11.3535 - val_accuracy: 0.9960\n",
      "Epoch 18/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.4105 - accuracy: 0.9706 - val_loss: 11.3084 - val_accuracy: 0.9971\n",
      "Epoch 19/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.3622 - accuracy: 0.9728 - val_loss: 11.2638 - val_accuracy: 0.9977\n",
      "Epoch 20/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.3112 - accuracy: 0.9759 - val_loss: 11.2207 - val_accuracy: 0.9983\n",
      "Epoch 21/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.2649 - accuracy: 0.9775 - val_loss: 11.1802 - val_accuracy: 0.9972\n",
      "Epoch 22/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.2194 - accuracy: 0.9785 - val_loss: 11.1368 - val_accuracy: 0.9977\n",
      "Epoch 23/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.1730 - accuracy: 0.9796 - val_loss: 11.0942 - val_accuracy: 0.9984\n",
      "Epoch 24/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.1263 - accuracy: 0.9827 - val_loss: 11.0532 - val_accuracy: 0.9977\n",
      "Epoch 25/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.0831 - accuracy: 0.9830 - val_loss: 11.0106 - val_accuracy: 0.9984\n",
      "Epoch 26/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 11.0401 - accuracy: 0.9831 - val_loss: 10.9695 - val_accuracy: 0.9985\n",
      "Epoch 27/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.9939 - accuracy: 0.9849 - val_loss: 10.9275 - val_accuracy: 0.9989\n",
      "Epoch 28/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.9485 - accuracy: 0.9864 - val_loss: 10.8869 - val_accuracy: 0.9985\n",
      "Epoch 29/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.9039 - accuracy: 0.9875 - val_loss: 10.8457 - val_accuracy: 0.9985\n",
      "Epoch 30/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.8628 - accuracy: 0.9880 - val_loss: 10.8040 - val_accuracy: 0.9991\n",
      "Epoch 31/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.8200 - accuracy: 0.9883 - val_loss: 10.7634 - val_accuracy: 0.9989\n",
      "Epoch 32/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.7809 - accuracy: 0.9867 - val_loss: 10.7227 - val_accuracy: 0.9989\n",
      "Epoch 33/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.7364 - accuracy: 0.9891 - val_loss: 10.6810 - val_accuracy: 0.9992\n",
      "Epoch 34/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6925 - accuracy: 0.9900 - val_loss: 10.6410 - val_accuracy: 0.9992\n",
      "Epoch 35/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6513 - accuracy: 0.9896 - val_loss: 10.6008 - val_accuracy: 0.9991\n",
      "Epoch 36/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.6100 - accuracy: 0.9896 - val_loss: 10.5603 - val_accuracy: 0.9991\n",
      "Epoch 37/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.5683 - accuracy: 0.9910 - val_loss: 10.5204 - val_accuracy: 0.9991\n",
      "Epoch 38/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.5289 - accuracy: 0.9910 - val_loss: 10.4807 - val_accuracy: 0.9989\n",
      "Epoch 39/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4850 - accuracy: 0.9920 - val_loss: 10.4401 - val_accuracy: 0.9991\n",
      "Epoch 40/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4472 - accuracy: 0.9920 - val_loss: 10.3998 - val_accuracy: 0.9991\n",
      "Epoch 41/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.4042 - accuracy: 0.9922 - val_loss: 10.3600 - val_accuracy: 0.9991\n",
      "Epoch 42/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.3633 - accuracy: 0.9927 - val_loss: 10.3205 - val_accuracy: 0.9989\n",
      "Epoch 43/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.3220 - accuracy: 0.9927 - val_loss: 10.2803 - val_accuracy: 0.9991\n",
      "Epoch 44/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2836 - accuracy: 0.9924 - val_loss: 10.2414 - val_accuracy: 0.9991\n",
      "Epoch 45/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2410 - accuracy: 0.9936 - val_loss: 10.2018 - val_accuracy: 0.9989\n",
      "Epoch 46/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.2043 - accuracy: 0.9932 - val_loss: 10.1623 - val_accuracy: 0.9989\n",
      "Epoch 47/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.1634 - accuracy: 0.9933 - val_loss: 10.1231 - val_accuracy: 0.9992\n",
      "Epoch 48/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.1233 - accuracy: 0.9937 - val_loss: 10.0832 - val_accuracy: 0.9993\n",
      "Epoch 49/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0828 - accuracy: 0.9936 - val_loss: 10.0440 - val_accuracy: 0.9992\n",
      "Epoch 50/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0448 - accuracy: 0.9936 - val_loss: 10.0054 - val_accuracy: 0.9991\n",
      "Epoch 51/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 10.0037 - accuracy: 0.9943 - val_loss: 9.9661 - val_accuracy: 0.9992\n",
      "Epoch 52/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.9641 - accuracy: 0.9945 - val_loss: 9.9274 - val_accuracy: 0.9992\n",
      "Epoch 53/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.9246 - accuracy: 0.9951 - val_loss: 9.8888 - val_accuracy: 0.9991\n",
      "Epoch 54/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8873 - accuracy: 0.9944 - val_loss: 9.8500 - val_accuracy: 0.9991\n",
      "Epoch 55/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8460 - accuracy: 0.9956 - val_loss: 9.8116 - val_accuracy: 0.9991\n",
      "Epoch 56/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.8079 - accuracy: 0.9947 - val_loss: 9.7727 - val_accuracy: 0.9992\n",
      "Epoch 57/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.7701 - accuracy: 0.9945 - val_loss: 9.7344 - val_accuracy: 0.9991\n",
      "Epoch 58/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.7288 - accuracy: 0.9962 - val_loss: 9.6961 - val_accuracy: 0.9991\n",
      "Epoch 59/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6915 - accuracy: 0.9951 - val_loss: 9.6576 - val_accuracy: 0.9992\n",
      "Epoch 60/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6535 - accuracy: 0.9951 - val_loss: 9.6193 - val_accuracy: 0.9995\n",
      "Epoch 61/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.6139 - accuracy: 0.9960 - val_loss: 9.5813 - val_accuracy: 0.9993\n",
      "Epoch 62/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5756 - accuracy: 0.9958 - val_loss: 9.5438 - val_accuracy: 0.9989\n",
      "Epoch 63/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.5385 - accuracy: 0.9952 - val_loss: 9.5050 - val_accuracy: 0.9995\n",
      "Epoch 64/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.4997 - accuracy: 0.9956 - val_loss: 9.4677 - val_accuracy: 0.9992\n",
      "Epoch 65/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.4623 - accuracy: 0.9959 - val_loss: 9.4301 - val_accuracy: 0.9992\n",
      "Epoch 66/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.4229 - accuracy: 0.9960 - val_loss: 9.3921 - val_accuracy: 0.9995\n",
      "Epoch 67/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3861 - accuracy: 0.9959 - val_loss: 9.3548 - val_accuracy: 0.9992\n",
      "Epoch 68/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3479 - accuracy: 0.9963 - val_loss: 9.3170 - val_accuracy: 0.9993\n",
      "Epoch 69/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.3110 - accuracy: 0.9964 - val_loss: 9.2799 - val_accuracy: 0.9993\n",
      "Epoch 70/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.2752 - accuracy: 0.9952 - val_loss: 9.2426 - val_accuracy: 0.9995\n",
      "Epoch 71/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.2364 - accuracy: 0.9961 - val_loss: 9.2051 - val_accuracy: 0.9996\n",
      "Epoch 72/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.1974 - accuracy: 0.9967 - val_loss: 9.1681 - val_accuracy: 0.9996\n",
      "Epoch 73/100\n",
      "90/90 [==============================] - 10s 116ms/step - loss: 9.1610 - accuracy: 0.9962 - val_loss: 9.1313 - val_accuracy: 0.9993\n",
      "Epoch 74/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.1232 - accuracy: 0.9968 - val_loss: 9.0942 - val_accuracy: 0.9993\n",
      "Epoch 75/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0857 - accuracy: 0.9968 - val_loss: 9.0574 - val_accuracy: 0.9993\n",
      "Epoch 76/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0490 - accuracy: 0.9964 - val_loss: 9.0205 - val_accuracy: 0.9993\n",
      "Epoch 77/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 9.0140 - accuracy: 0.9965 - val_loss: 8.9842 - val_accuracy: 0.9993\n",
      "Epoch 78/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9744 - accuracy: 0.9976 - val_loss: 8.9475 - val_accuracy: 0.9993\n",
      "Epoch 79/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9396 - accuracy: 0.9964 - val_loss: 8.9107 - val_accuracy: 0.9993\n",
      "Epoch 80/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.9013 - accuracy: 0.9970 - val_loss: 8.8742 - val_accuracy: 0.9995\n",
      "Epoch 81/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.8653 - accuracy: 0.9969 - val_loss: 8.8381 - val_accuracy: 0.9993\n",
      "Epoch 82/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.8283 - accuracy: 0.9974 - val_loss: 8.8014 - val_accuracy: 0.9995\n",
      "Epoch 83/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7929 - accuracy: 0.9972 - val_loss: 8.7656 - val_accuracy: 0.9993\n",
      "Epoch 84/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7564 - accuracy: 0.9972 - val_loss: 8.7289 - val_accuracy: 0.9995\n",
      "Epoch 85/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.7205 - accuracy: 0.9968 - val_loss: 8.6935 - val_accuracy: 0.9995\n",
      "Epoch 86/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6851 - accuracy: 0.9966 - val_loss: 8.6575 - val_accuracy: 0.9996\n",
      "Epoch 87/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6483 - accuracy: 0.9972 - val_loss: 8.6214 - val_accuracy: 0.9996\n",
      "Epoch 88/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.6118 - accuracy: 0.9975 - val_loss: 8.5860 - val_accuracy: 0.9993\n",
      "Epoch 89/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5762 - accuracy: 0.9970 - val_loss: 8.5498 - val_accuracy: 0.9995\n",
      "Epoch 90/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5400 - accuracy: 0.9977 - val_loss: 8.5144 - val_accuracy: 0.9995\n",
      "Epoch 91/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.5039 - accuracy: 0.9977 - val_loss: 8.4794 - val_accuracy: 0.9993\n",
      "Epoch 92/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4698 - accuracy: 0.9971 - val_loss: 8.4439 - val_accuracy: 0.9993\n",
      "Epoch 93/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.4328 - accuracy: 0.9978 - val_loss: 8.4082 - val_accuracy: 0.9993\n",
      "Epoch 94/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3980 - accuracy: 0.9976 - val_loss: 8.3732 - val_accuracy: 0.9995\n",
      "Epoch 95/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3627 - accuracy: 0.9976 - val_loss: 8.3378 - val_accuracy: 0.9992\n",
      "Epoch 96/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.3286 - accuracy: 0.9976 - val_loss: 8.3028 - val_accuracy: 0.9993\n",
      "Epoch 97/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2930 - accuracy: 0.9972 - val_loss: 8.2678 - val_accuracy: 0.9995\n",
      "Epoch 98/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2570 - accuracy: 0.9979 - val_loss: 8.2327 - val_accuracy: 0.9992\n",
      "Epoch 99/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.2219 - accuracy: 0.9981 - val_loss: 8.1982 - val_accuracy: 0.9992\n",
      "Epoch 100/100\n",
      "90/90 [==============================] - 10s 115ms/step - loss: 8.1895 - accuracy: 0.9974 - val_loss: 8.1634 - val_accuracy: 0.9995\n",
      "Time: 1037.1756384372711\n",
      "INFO:tensorflow:Assets written to: ./Trained_Models/Trained_Comb_Model_100disc_1KSNPs.acc.mod/assets\n"
     ]
    }
   ],
   "source": [
    "################################################################################################################################################\n",
    "#100 discrete, 1K SNPs\n",
    "################################################################################################################################################\n",
    "\n",
    "# Create the MLP, the CNN and the combined models\n",
    "mlp = create_mlp(traits_disc_train)\n",
    "cnn = create_cnn(xtest)\n",
    "combinedInput = LinearW()([mlp.output, cnn.output])\n",
    "\n",
    "# The final fully-connected layer head will have two dense layers (one relu and one softmax)\n",
    "x = Dense(50, activation=\"relu\")(combinedInput)\n",
    "x = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# The final model accepts numerical data on the MLP input and images on the CNN input, outputting a single value\n",
    "model = Model(inputs=[mlp.input, cnn.input], outputs=x)\n",
    "\n",
    "# using Stochastic Gradient Descent as optimizer and a categorical cross-entropy loss function\n",
    "opt = SGD(learning_rate=0.001)\n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "\t              optimizer=opt,\n",
    "\t              metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())\n",
    "\n",
    "# save only the epoch with the highest accuracy in the validation set, by using the model checkpoint\n",
    "earlyStopping = EarlyStopping(monitor='val_accuracy', patience=150, verbose=0, mode='max', restore_best_weights=True)\n",
    "\n",
    "# fit the model and record running times\n",
    "start = time.time()\n",
    "model.fit([traits_disc_train, xtrain], ytrain, batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          verbose=1,\n",
    "          validation_data=([traits_disc_test, xtest], ytest),callbacks=[earlyStopping])\n",
    "print (f'Time: {time.time() - start}')\n",
    "\n",
    "# save the model\n",
    "model.save(filepath='./Trained_Models/Trained_Comb_Model_100disc_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "CJn4Q4eRnWO0",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that the models are trained, we will evaluate their accuracy based on the test set. For that, we will build confusion matrices containing the true and predicted scenarions for each simulation on the test set.\n",
    "\n",
    "# first import the libraries\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "from keras.models import load_model\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# load the trained models.\n",
    "model1 = load_model('./Trained_Models/Trained_Comb_Model_100BM_1KSNPs.acc.mod')\n",
    "model2 = load_model('./Trained_Models/Trained_Comb_Model_100OU_1KSNPs.acc.mod')\n",
    "model3 = load_model('./Trained_Models/Trained_Comb_Model_100disc_1KSNPs.acc.mod')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jF6nrDm1vH0x",
    "outputId": "b29ca1db-ffd1-41af-fe19-f57e77cc2e17",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load the traits simulated under the BM model for the 3 scenarios.\n",
    "traits_BM = []\n",
    "traits_BM = np.loadtxt(\"./testSims/traits/traits_BM.txt\").reshape(3000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_BM = np.array(traits_BM)\n",
    "\n",
    "#Use standard scaling for the continuous (BM) traits.\n",
    "for i in range(traits_BM.shape[2]):\n",
    "    traits_BM[:, :, i] = scalers_BM[i].transform(traits_BM[:, :, i]) \n",
    "\n",
    "# load the traits simulated under the BM model for the 3 scenarios.\n",
    "traits_OU = []\n",
    "traits_OU = np.loadtxt(\"./testSims/traits/traits_OU.txt\").reshape(3000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_OU = np.array(traits_OU)\n",
    "\n",
    "#Use standard scaling for the continuous (OU) traits.\n",
    "for i in range(traits_OU.shape[2]):\n",
    "    traits_OU[:, :, i] = scalers_OU[i].transform(traits_OU[:, :, i]) \n",
    "\n",
    "# load the discrete traits simulated for the 3 scenarios.\n",
    "traits_disc = []\n",
    "traits_disc = np.loadtxt(\"./testSims/traits/traits_disc.txt\").reshape(3000,-1,100)\n",
    "# transform into a NumPy array. \n",
    "traits_disc = np.array(traits_disc)\n",
    "\n",
    "# load the SNPs simulated for the 3 scenarios. \n",
    "u1 = np.load(\"./testSims/Model_1sp.npz\",mmap_mode='r')\n",
    "u2 = np.load(\"./testSims/Model_2sp.npz\",mmap_mode='r')\n",
    "u3 = np.load(\"./testSims/Model_3sp.npz\",mmap_mode='r')\n",
    "\n",
    "# combine the loaded SNPs in a single NumPy array.\n",
    "xtest=np.concatenate((u1['Model_1sp'],u2['Model_2sp'],u3['Model_3sp']),axis=0)\n",
    "\n",
    "#transform major alleles in -1 and minor in 1\n",
    "for arr,array in enumerate(xtest):\n",
    "    for idx,row in enumerate(array):\n",
    "        if np.count_nonzero(row) > len(row)/2:\n",
    "            xtest[arr][idx][xtest[arr][idx] == 1] = -1\n",
    "            xtest[arr][idx][xtest[arr][idx] == 0] = 1\n",
    "        else:\n",
    "            xtest[arr][idx][xtest[arr][idx] == 0] = -1\n",
    "\n",
    "# create a label vector in the same order as the simulations.\n",
    "ytest=[0 for i in range(len(u1['Model_1sp']))]\n",
    "ytest.extend([1 for i in range(len(u2['Model_2sp']))])\n",
    "ytest.extend([2 for i in range(len(u3['Model_3sp']))])\n",
    "ytest = np.array(ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 705
    },
    "id": "qfi_m7crw2Xo",
    "outputId": "d8eaef70-88e1-432a-f262-30b3f61b096d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#define a funtion to build the confusion matrix\n",
    "def makeConfusionMatrixHeatmap(data, title, trueClassOrderLs, predictedClassOrderLs, ax):\n",
    "    data = np.array(data)\n",
    "    data = normalize(data, axis=1, norm='l1')\n",
    "    heatmap = ax.pcolor(data, cmap=plt.cm.Blues, vmin=0.0, vmax=1.0)\n",
    "\n",
    "    for i in range(len(predictedClassOrderLs)):\n",
    "        for j in reversed(range(len(trueClassOrderLs))):\n",
    "            val = 100*data[j, i]\n",
    "            if val > 50:\n",
    "                c = '0.9'\n",
    "            else:\n",
    "                c = 'black'\n",
    "            ax.text(i + 0.5, j + 0.5, '%.2f%%' % val, horizontalalignment='center', verticalalignment='center', color=c, fontsize=9)\n",
    "\n",
    "    cbar = plt.colorbar(heatmap, cmap=plt.cm.Blues, ax=ax)\n",
    "    cbar.set_label(\"Fraction of simulations assigned to class\", rotation=270, labelpad=20, fontsize=11)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_xticks(np.arange(data.shape[1]) + 0.5, minor=False)\n",
    "    ax.set_yticks(np.arange(data.shape[0]) + 0.5, minor=False)\n",
    "    ax.axis('tight')\n",
    "    ax.set_title(title)\n",
    "\n",
    "    #labels\n",
    "    ax.set_xticklabels(predictedClassOrderLs, minor=False, fontsize=9, rotation=45)\n",
    "    ax.set_yticklabels(reversed(trueClassOrderLs), minor=False, fontsize=9)\n",
    "    ax.set_xlabel(\"Predicted class\")\n",
    "    ax.set_ylabel(\"True class\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 601
    },
    "id": "oVjUXRySZGmu",
    "outputId": "5514c41c-7ab3-4011-f97f-03b9c8e79bcd",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000    0    0]\n",
      " [   0 1000    0]\n",
      " [   0    2  998]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1594336/1503990667.py:16: MatplotlibDeprecationWarning: The 'cmap' parameter to Colorbar has no effect because it is overridden by the mappable; it is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  cbar = plt.colorbar(heatmap, cmap=plt.cm.Blues, ax=ax)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY0AAAEkCAYAAADTtG33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA7BUlEQVR4nO3deXwV1d3H8c83gQAmSIJ7ERRcgFZRAVGRsLqAS2mfuiEiohWotXVBqe1jXepaFreqVdxARMGtfWyxgiKURUUWEdxQRAUUNwJhDevv+WMm4RKSm5tMlnvD7+3rvnJn5pyZM5d4fznLnCMzwznnnEtEWk0XwDnnXOrwoOGccy5hHjScc84lzIOGc865hHnQcM45lzAPGs455xLmQcPVGEkNJP1LUr6kFyKcp6+kyZVZtpoiKVfS4pouh3OlkT+n4coi6ULgWqAVsA5YANxhZjMjnrcf8Dugo5lti1rOZCfJgCPMbElNl8W5ivKahotL0rXAfcCdwAFAM+BhoHclnP4Q4NM9IWAkQlKdmi6Dc2XxoOFKJakR8Bfgt2b2spltMLOtZvYvM7s+TFNP0n2Svglf90mqFx7rKmmFpCGSvpe0UtKA8NitwE3A+ZLWS7pM0i2Snom5/qGSrPDLVNIlkpZKWifpC0l9Y/bPjMnXUdKcsNlrjqSOMcemSbpN0qzwPJMl7VvK/ReWf2hM+X8h6QxJn0rKk/SnmPQdJL0taU2Y9kFJGeGx6WGy98P7PT/m/H+Q9C3wVOG+MM9h4TXahts/kfSjpK5R/l2di8KDhovnJKA+8I84af4XOBE4FjgG6ADcGHP8QKAR0AS4DHhIUo6Z3UxQe5lgZllm9kS8gkjKBB4AeplZQ6AjQTNZ8XSNgYlh2n2Ae4CJkvaJSXYhMADYH8gArotz6QMJPoMmBEHuMeAioB2QC9wkqUWYdjtwDbAvwWfXA7gCwMw6h2mOCe93Qsz5GxPUugbGXtjMPgf+AIyTtBfwFDDazKbFKa9zVcqDhotnH+DHMpqP+gJ/MbPvzewH4FagX8zxreHxrWb2KrAeaFnB8uwAjpLUwMxWmtmHJaQ5E/jMzMaa2TYzew74BDg7Js1TZvapmW0CnicIeKXZStB/sxUYTxAQ7jezdeH1PwTaAJjZPDN7J7zul8CjQJcE7ulmM9sclmcXZvYY8BkwGziIIEg7V2M8aLh4VgH7ltHW/hPgq5jtr8J9RecoFnQ2AlnlLYiZbQDOBwYDKyVNlNQqgfIUlqlJzPa35SjPKjPbHr4v/FL/Lub4psL8ko6U9G9J30paS1CTKrHpK8YPZlZQRprHgKOAv5nZ5jLSOlelPGi4eN4GCoBfxEnzDUHTSqFm4b6K2ADsFbN9YOxBM5tkZqcS/MX9CcGXaVnlKSzT1xUsU3n8naBcR5jZ3sCfAJWRJ+7wRUlZBAMRngBuCZvfnKsxHjRcqcwsn6Ad/6GwA3gvSXUl9ZI0LEz2HHCjpP3CDuWbgGdKO2cZFgCdJTULO+H/WHhA0gGSfh72bWwmaObaXsI5XgWOlHShpDqSzgd+Cvy7gmUqj4bAWmB9WAv6TbHj3wEtdssV3/3APDP7NUFfzSORS+lcBB40XFxmdg/BMxo3Aj8Ay4ErgX+GSW4H5gILgUXA/HBfRa71OjAhPNc8dv2iTwOGENQk8gj6Cq4o4RyrgLPCtKuAocBZZvZjRcpUTtcRdLKvI6gFTSh2/BZgTDi66ryyTiapN9CToEkOgn+HtoWjxpyrCf5wn3POuYR5TcM551zCPGg451ySkPRk+CDpB6Ucl6QHJC2RtLDwwc/q5EHDOeeSx2iCfqzS9AKOCF8DCUbsVSsPGs45lyTMbDrBQI/S9AaetsA7QLakg6qndAGfIM0551JHE4IRjIVWhPtWVuRkkpZS+rNEMrNDi++sdUFDdRqYMhrWdDFqteNaN6vpIjhXKebPn/ejme0X5Rzpex9itm23GWBKZJt++JDggdlCo8xsVDkuV9IXfJQhsGcVO89LwDkx73dT+4JGRkPqtSxzCLyLYNbsB2u6CM5VigZ1VXzKmXKzbQXUa3VBQmkL3vtbgZm1j3C5FUDTmO2DqfgMDJjZR7HbkjYX7pNU4pQ13qfhnHNRCEhLT+wV3SvAxeEoqhOBfDOrUNNUKayU90VqXU3DOeeqncqaYizR0+g5oCvBRKErgJuBugBm9gjBNDlnAEsIJtscUCkX3ukPMe+nlpTAg4ZzzkUiUOU02phZnzKOG/DbSrkYIKl/SfvMbIyZDSkpjwcN55yLqpJqGjXgzJj3WUAnYBYwprQMHjSccy4KUWk1jepmZruMGpJ0KMESz6XyoOGcc5EolWsauzCzLyW1jpfGg4ZzzkVVOSOjaoSkhkBBuKQxwGWS0sxsR0npU7NO5ZxzSSPsCE/klWQkXUewOFiepJ6S9gFOKS1ggAcN55yLRgTNU4m8ks9vCR4W7AT8MVzELO6Tit485ZxzUSVhLSJBX4WBYlXM+vNx29pS9k6dcy45pG7zFPAfSbeHM+XukNSDXefG2o3XNJxzLqq0pGx6SsSd4c8/ApuB24FB8TJ40HDOuSgK555KQWZW7oJ70HDOuUgqbxqRmiApB+hIMEHh22a2Ol56DxrOORdVco6MKlM4U+7LQOEU6T+T9D9m9nZpeTxoOOdcVKlb07gH+KWZzQaQdAIwAsgtLYMHDeeciyJ5n8FIRGZhwAAws9nhE+Kl8qDhnHNRpWhHOLA9dsoQSaKM5WM9aDjnXCQp3RF+HbA3sCbc3hu4Pl6GlL1T55xLGik6jYiZvWlma2K284EO8fJ40HDOuSgK19NIwSfCJV0maYGkLwpfwM3h+6tKyuPNU845F0lKN08NBS4B8sNtA14CzgG+LymDBw3nnIsqCZueErSh+DMZkgrM7KPSMnjQcM65qFJ39NSFCe4r4kHDOeeiUEo3T52vkmtJt0oaZGaPFj/gQcM556JK3eapzBL2Fd5M/ZIyeNBwzrmISvlrPemZ2dA4x+4vab8HDeeciyBY7TU1g4akNGAgcCrByKkpwKPx1gj3oOGcc1GInQ06qeevQBtgNMFdXAIcRvCkeIk8aDjnXCQiLS1lO8J7AseZ2TYASROABcQJGil7pzVt9G0XMOe5q/ntBScX7bt58GlMGNaPx285j0ZZQR9So6z6PH7LeUwY1o+bB59W4rk6t2vBiyP78+LI/uS2bVG0/4rzOvL8iIt55q6+NNm/EQC/OqUN/7h3ALdf2aso3V1Xnck+jfaqittMSmPHjKZrbke6dT6Z9+bP3+VYQUEBl/TrS4+uuVzSry8FBcFyx199+SU9T+1Ot84nM+zuYIXLDRs20Ou0HnQ6qQML338fgEULF3LrzX+u3htKUv45J05SQq8kZOxaTypzwkIPGhV0w30TufuJN4u2O7drQYN6dTl/6FgmTv+YQeecBMCgc07i39M/4vyhY9mrfgad27XY5TxpaeKGS7sz4KbxDLhpPH+8rDtpaaLFwftw0jGHct51T3P/uOkMHdANgD69juPc68bQ7KBsGmXV56RjDuHjpd+xKn9j9d18DVq9ejUPP/gAk6dM46kxzzDkmt/vcnzsmNG0bNWKKdNmcGTLlowdMxqAG//3Bm68+VamTp/FtKlvsviTT3jj9cl0696DYSPuZczoJwG4Z8Qwrht6Q3XfVtLxz7l8UjhovAZMlNRXUt9we1K8DB40KujbVet22T7x6EN4890lAEx59zOOP6oZACe0idk/+zM6hPsLHfqTxiz/Np91GzazbsNmln+bzyEH5XBim0OYOifIN+eD5bRufgAABVu2kZ6eRnpaGjt2GOeffizPTJxXpfeaTOa8O5uOnXLJyMjg0ObN2bB+PZs3by46Pn36NHqdcRYAZ5x5NjNnTgdg4fsL6NQpWFemZ68zmTljOpmZmRQUFLBp00aysrKYMP45zu79CzIzSxqFuGfxz7kcVI5X8vkD8ALQG/hF+L7UEVVQQ0FDgUclzZT0lqQOkkZLelDSREnvSNo/THuupBlh2ptqoryJaNSwAfnrNwGwdn0B2Q13Nk+tXR9U3dduKCC7YYNd8mU3rF+ULzZNdlZ98sN8AOnpwW/cPU9PY9jVZ/HKtA/51altGPfqfH7XpxM3DTqNZgdmV+UtJoW8vDxycnKKtvdu1Ii8vLyi7dUxx7Ozs8lbtQqAHTt2DgbJzs4mL28V3XucwsaNGxn/7Dgu7j+ANyZPomnTZgy55ioeuO/earqj5OSfc+JEYrWMZKxpWOAxMzvPzM41s0fNLCmbp3oDdc2sE3AR8GC4f4mZnQm8ApwXLng+BOgepj1O0tHFTyZpoKS5kubatk3FD1eL/HWb2Dvsx2iYWa/oCz9/fQENM+vF7N+1fGvWFRTlK0yzZt0m1qwvYO8wH8D27cG/4/yPv+aqYf/kjXc+pdmBOdSrW4eCzdt4+PlZXH1R5yq9x2TQuHFj1qxZU7S9Nj+fxo0bF23nxBzPz88nJzwW21GZn59PTk5j0tLSuHvYCB57cjTPjhvLdUNv4I7bbuGuvw5nyWef8vmSJdVyT8nIP+fySUtLS+iVbCQ9Kemp4q94eWrqLloCbwGY2VKg8E+awnaWZcA+wOHAIcDrkqYBzcPtXZjZKDNrb2btVadB8cPVYvYHy+ja/nAAuh1/OO8uWgbAu4u+otvxwf6u7Q9ndri/0Jff5NH0gGyyGmSQ1SCDpgdk89XK1cxe9BVd2h8GQNvWTfj4i+92yfeb8zry8POz2Kt+Bhl108mok05mg3rUdsd3OIG3Z81k69atLFu2jMysLOrV23nfubldmPTaqwBMeu1VcnO7AHB0m2N4+623AJg86T90yt0ZYD9fsgQzo2WrVuTl5WFmbN68mXXrdm2C3JP451w+qVrTAOYCc8LXIoLhtgXxMtTUkNvFwM+BxyW1YOeqUbHVIgFLgSXAKWa2LXwQJSk++Tt/fwZtWx9MRt10jj7iIH5z+4t073A4E4b1Y/3GLQwZ+QoAj774DiOH/Jy+Z7Tlky+/Z8b8pQD8eeCpPDR+FnlrNzJ89FRG394HgOGjp7Jjh/H58lXM/WgFz4+4mK1bt3PD/ROLrt3myJ+w/Ls1/Lh6AzPmL+Xis9txyolHMnz01Or/IKpZTk4OAwdfwanduyCJEffcz/sLFjBlyutcO+R6+vW/hEGXX0qPrrk0OfhgRj0e/NF02+13MXjgZWzZsoXTe/aiVevWRee8d+Rw7h4+EoBBg68oynvMscfWxC0mBf+cyyF5+yvKZGYPx25L+htBZ3ipVEbzVZUIv/wfBVoD6cA1wGDgcTObKeki4HAzu0XSr4CrgO3AVuBiM/u2tHOn7bW/1Wt5XpXfw55s9ZwHy07kXApoUFfzzKx9lHPU2beFZZ91Z0JpV43pE/l6VUlSXeBDMzuytDQ1UtMIH1G/vNjud2KOPxPz/iWCRUGccy7pFHaEV8q5pJ7A/QR/TD9uZncXO94IeAZoRvD9PcLM4vZBlHG9J9lZT0oH2hJ2HZTGnwh3zrmIKiNoSEoHHiKYB2oFMEfSK8UWRPot8JGZnS1pP2CxpHFmtqWCl50b834bMMbMpsTL4EHDOeeiECitUmoaHQhGkC4FkDSeYKRpbNAwoKGCKJUF5BF82VdI8T6NRHjQcM65iMpR09hXUuxf96PMbFT4vgmwPObYCuCEYvkfJHgk4RugIXB+vBlpq4IHDeeci6gcQePHOB3hJZ2k+Eil0wkmFOxOMDz2dUkzzGxtogWIKvmeNnHOuRRSiU+ErwCaxmwfTFCjiDUAeDl8knsJ8AXQqtJuJgFe03DOuagqZ/DUHOAISc2Br4ELgAuLpVkG9ABmSDqA4EHppVEuKumn4TkNmGpmH8ZL7zUN55yLQpXzRHi4psWVBLPMfgw8b2YfShosaXCY7Dago6RFBKvs/cHMfqxw0YNn4iYBRxEsxjRZ0sXx8nhNwznnIqqseaXM7FXg1WL7Hol5/w1Q8sI8FTMUaGdm3wOEE8W+ATxdWgYPGs45F1WKTiMC7CgMGABm9r2kuKOxPGg451xESToZYSKWSroVKBz2Owj4PF4G79NwzrkIEu3PSNLAMgg4AngPeB84MtxXKq9pOOdcREkaEMpkZj9QbISWpKx4eTxoOOdcRJU0jUi1k7Tb+kTAq5K6m9l3JRzzoOGcc1Glak2D4NkQseuT59nAp5JeNrMBxTN40HDOuSiUukHDzPYvvk/SfDNrGz4LshsPGs45F4GAFI0ZpRkT/vygpIMeNJxzLpKkHRlVIWZ2f/izT0nHPWg451xEtShmlMmDhnPORSFIS9HRUxXhD/c551wEIggaibySjaTjJO0bvt9b0rEqo63Ng4ZzzkUkJfZKQo8B2yRlAPOACQTrlJfKg4ZzzkWUwtOIpJvZGqArMN3MWobvS+V9Gs45F0Xy1iISUUdSGnAKMDXctzluhiovknPO1WJClbaeRg14DVhE8BT4nZIaAevjZfCg4ZxzEaVqTcPMrpf0ErA0bKYCyI2Xx4OGc85FlKT9FWUKJyxcCTSInbzQzL6SdJCZrSyex4OGc85Fkdp9GiVNWChgP+AZoEfxDB40nHMugmDuqdSMGiVNWBhzbLeAAR40nHMushSNGRXiQcM55yJKxqe9EyFpOzubp4puwsxKHQ7mQcM556JI4fU0gIYx7+sD5wGN42VI2cHFzjmXDArX00jFaUTMbGPMK8/MHgF+ES9PratpHNe6GbNmP1jTxajVco6/sqaLUOutnuO/w6kjaacIKVOxNcLTgbaUUdOodUHDOeeqW4rGDNh1yG09gtan3vEyeNBwzrmIUrWmUXzIraSeBPNQvVlaHu/TcM65CKTUXU+jODN7DegZL43XNJxzLqJUrWlI6hKzmQ60o4y44EHDOeciStGYATA85v02YAlwbrwMHjSccy6iVK1pmFmH8ubxoOGcc1Ek6TMYiZJ0InAYMfHAzMaUlt6DhnPORRAswpSaUUPSwwSjpRYCOwp3Ax40nHOuqqSlblWjB/AzM9uaaAYfcuuccxFV1jQiknpKWixpiaQbSknTVdICSR9K+m/Eon9BzESFifCahnPORaBKmrBQUjrwEHAqsAKYI+kVM/soJk028DDQ08yWSSp1PYwELQYmSnoRKCjc6X0azjlXhSqpS6MDsMTMlgJIGk8wpcdHMWkuBF42s2UAZvZ9xGseBKxm1xX6ovVpSDoXeM3M1km6kWBCq9vNbH7EwjrnXK1QSUNumwDLY7ZXACcUS3MkUFfSNIJpze83s6crekEzO6+8eRKpafzZzF6Q1Ak4HRgB/J3db8Y55/Y4olwd4ftKmhuzPcrMRsWcqjgrtl2H4KntHkAD4G1J75jZp+UochFJ/eMdL6mZKpGgsT38eSbwdzP7P0m3lL94zjlXO5WjeepHM2tfyrEVQNOY7YOBb0pI86OZbQA2SJoOHANUKGgQfK+XpsRmqkSCxteSHiUYy/tXSYXT5zrnnFOlracxBzhCUnPga+ACgj6MWP8HPCipDpBB0OJzb0UvWFXNU+cRzHo4wszWSDoIuL68F3LOudqqMmKGmW2TdCUwiWDywCfN7ENJg8Pjj5jZx5JeY+fDeI+b2QcVL7eaAb8H1gD3ELQsZZvZd6XlSSRoHARMNLPNkroCbYAKd7w451xtUs4+jbjM7FXg1WL7Him2PZxdJxqM4gVgJvBTgv7q64DngO6lZUikmeklYLukw4EngObAs5GL6pxztUSqrhEO1DGzIUB/oKOZbSQYlVWqRILGDjPbBvwPcJ+ZXUNQ+3DOuT1eii/CtFxSk3AaEYV9JfXjZUikeWqrpD7AxcDZ4b660crpnHO1RwrPPbUemCfp/4ADCPpTJsbLkEjQGAAMBu4wsy/Cnv1nopbUOedqi5QNGcFQ3cLhuvcAC8xscrwMZQaNcN6T38dsfwHcHaGQzjlXq6TwIkx/Kb5P0lHxRmQlMo3IEcBdBL3rRW1dZtaiguV0zrlaIxg9VdOlqBhJhwK/BPaO2T1Y0iPANDPbbRbdRJqnngJuJniApBtBc1WKfkTOOVfJlLSd3Il4meChwvyYfQKyCB4e3E0iQaOBmU2RJDP7CrhF0gyCQOKcc3u8VG2eAjCzQbHbkk4xs1If4E4kaBRISgM+C59W/BqIOoe7c87VCqncPAWMT3BfkUSCxtXAXgSd4bcRPCkYd2ZE55zbk6RwTWOCpEOK7wOQdJCZrSyeIZHRU3PCt+sJ+jOcc87FSNmQEfRniF2nYBewH8GjFT2KZyg1aEj6F7vP5V7EzH5e4WI651wtIaXuw31mVmpXg5ntFjAg/jQiI4CRcV6uBGPHjKZrbke6dT6Z9+bvurhhQUEBl/TrS4+uuVzSry8FBcGSvF99+SU9T+1Ot84nM+zuOwHYsGEDvU7rQaeTOrDw/fcBWLRwIbfe/OfqvaEaNPq2C5jz3NX89oKTi/bdPPg0Jgzrx+O3nEejrGAEeKOs+jx+y3lMGNaPmwefVuK5OrdrwYsj+/PiyP7ktt05WvyK8zry/IiLeeauvjTZvxEAvzqlDf+4dwC3X9mrKN1dV53JPo32qorbTFr+u5y4FJ5GBEn7Sjpb0lmJrDleatAws/+GY3TnAjNitmcSVGmiFDJb0sVRzpGMVq9ezcMPPsDkKdN4aswzDLnm97scHztmNC1btWLKtBkc2bIlY8eMBuDG/72BG2++lanTZzFt6pss/uQT3nh9Mt2692DYiHsZM/pJAO4ZMYzrht5Q3bdVY264byJ3P/Fm0Xbndi1oUK8u5w8dy8TpHzPonJMAGHTOSfx7+kecP3Qse9XPoHO7XR8hSksTN1zanQE3jWfATeP542XdSUsTLQ7eh5OOOZTzrnua+8dNZ+iAbgD06XUc5143hmYHZdMoqz4nHXMIHy/9jlX5G6vv5muY/y6XT6pOWCjpNOBD4EqCfusPJPWMlyeRCQunEHSEF2oAvFHRQoayCeayqlXmvDubjp1yycjI4NDmzdmwfj2bN28uOj59+jR6nXEWAGeceTYzZ04HYOH7C+jUKReAnr3OZOaM6WRmZlJQUMCmTRvJyspiwvjnOLv3L8jMzKz+G6sh365at8v2iUcfwpvvLgFgyrufcfxRzQA4oU3M/tmf0SHcX+jQnzRm+bf5rNuwmXUbNrP823wOOSiHE9scwtQ5Qb45HyyndfMDACjYso309DTS09LYscM4//RjeWbivCq912Tjv8uJEyJNib2S0F1ArpmdbmanAbnAnfEyJBI06pvZ+sKN8H3Uevq1QDtJ0yS9JyktrB6tBJB0rqQ/KfCopJmS3pLUIeJ1q1ReXh45OTlF23s3akReXl7R9uqY49nZ2eStWgXAjh07itJkZ2eTl7eK7j1OYePGjYx/dhwX9x/AG5Mn0bRpM4ZccxUP3FfhhbpSWqOGDchfvwmAtesLyG64s3lq7fqgeWTthgKyGzbYJV92w/pF+WLTZGfVJz/MB5CeHvxPfc/T0xh29Vm8Mu1DfnVqG8a9Op/f9enETYNOo9mB2VV5i0nDf5fLIcFaRnLGDNJj1xc3s8WUERcSCRobJLUt3JDUDtgUJ30i7gHmmVlXYD5wHMFQ3ncl/Sx8PxXoDdQ1s07ARcCDEa9bpRo3bsyaNWuKttfm59O4ceOi7ZyY4/n5+eSEx9LSdv4z5Ofnk5PTmLS0NO4eNoLHnhzNs+PGct3QG7jjtlu466/DWfLZp3y+ZEm13FMyyV+3ib3DfoyGmfWKvvDz1xfQMLNezP5dfz3XrCsoyleYZs26TaxZX8DeYT6A7duDcR/zP/6aq4b9kzfe+ZRmB+ZQr24dCjZv4+HnZ3H1RZ2r9B6Thf8ul4/CJV/LeiWhHyQN0E6XAj/Ey5BI0LgaeEHSjPBJ8AkE7V+VZQrBsK4jgYfC9+0J+k1aAm8BmNlSIKekE0gaKGmupLk//Bj3fqvU8R1O4O1ZM9m6dSvLli0jMyuLevV2finl5nZh0mvBolyTXnuV3NwuABzd5hjefustACZP+g+dcnd+MX2+ZAlmRstWrcjLy8PM2Lx5M+vW7dp0syeY/cEyurY/HIBuxx/Ou4uWAfDuoq/odnywv2v7w5kd7i/05Td5ND0gm6wGGWQ1yKDpAdl8tXI1sxd9RZf2hwHQtnUTPv5i1xUuf3NeRx5+fhZ71c8go246GXXSyWxQjz2B/y6XT1qCryQ0CLgc2EhQGRgY7itVQs9pSGpF8AUu4JNwwY4otsRc+03gFeBjgk72PwPfh+vlLgZ+DjwuqQXBOrYllXEUMAqgXbv2pQ4Trmo5OTkMHHwFp3bvgiRG3HM/7y9YwJQpr3PtkOvp1/8SBl1+KT265tLk4IMZ9fhTANx2+10MHngZW7Zs4fSevWjVunXROe8dOZy7hweD1QYNvqIo7zHHHlsTt1it7vz9GbRtfTAZddM5+oiD+M3tL9K9w+FMGNaP9Ru3MGTkKwA8+uI7jBzyc/qe0ZZPvvyeGfOXAvDngafy0PhZ5K3dyPDRUxl9ex8Aho+eyo4dxufLVzH3oxU8P+Jitm7dzg3371xGoM2RP2H5d2v4cfUGZsxfysVnt+OUE49k+Oip1f9B1AD/XU6cgPQkHRlVlvCP8Y6SMsPtDWXlkVn1f8eG05JMJIhuDwMPACPM7ClJ/wX+ZWYjwnSPAq0JFlq/xszeiXfudu3a26zZc6v2BvZwOcdXZkXTlWT1nKRuia01GtTVPDNrH+UcBxx+lPW958WE0t7bu3Xk61UmSV1K2l/S7LaFEplGpNKZ2Q6gV8yun8Uc61Is3eXVWDTnnCuXoJM7NWsawPCY9/UJWpQ+IuhnLlGNBA3nnKtNUrR1CjPbZUSqpDbAFfHylNk3E/aoXyTppnC7WbIPfXXOueqUwkNud2FmC4GT4qVJpKbxMLCDYBjsX4B1wEvA8VEL6JxzqU5AnVSICCUo1qeRDpxI8H1fqkSCxglm1lbSewBmtlpSiSs6OefcnihFYwbs2qexDfgcuCBehkSCxlZJ6YQz3krajzIikXPO7SmUvFOElKl4n0YiEnne5AHgH8D+ku4geJYi7twkzjm3J0nVPg1Jf5R0WPj+fyTdJ+nIeHnKDBpmNg4YSjCx1UrgF2b2QmUU2DnnaoM0JfZKQn2BpZIOJGiq+gEYHS9Dmc1TkpoRPIT3r9h9Zras9FzOObdnCNYIT86IkIAtZmbhFOnjzOwOSefEy5BIn8ZEgv4METz80RxYTMwDec45t8cSpCfpxFIJ2CGpI0GN4+5wX3q8DInMPXV07HY4423cCa2cc25PotRdJfxPwJPAHDObKqkRUZunijOz+ZL8GQ3nnKOweaqmS1ExZjYZaBWznU+wdEWpEunTuDZmMw1oSxnzrTvn3J4kVYNGRSRS02gY834bQR/HS1VTHOecSz0pPGFhucUNGuFDfVlmdn01lcc551KKUrsjvNxKvVVJdcxsO0FzlHPOuVKkhU+Fl/Uqi6SekhZLWiLphjjpjpe0vazhsQlcL13SWZI6JZonXk3jXYKAsUDSK8ALQNGqTmb2coVL6pxztURldYSHLTsPAacCK4A5kl4xs49KSPdXYFL0qzIOaAFkS3qEYOTUA2Z2UWkZEunTaAysIpjltvB5DQM8aDjnHJU2RUgHYEm4BCuSxgO9CRZFivU7Km+m8WMJVkbNASab2T1lTSMSL2jsH46c+oCdwaJQja3D7ZxzyUWkJf6cxr6SYtejHmVmo8L3TYDlMcdWACfsciWpCfBLgj/iKyNorAAyzCwvZvbyevEyxAsa6UAWlPhpeNBwzjmCL8hy1DR+jLNGeCLftfcBfzCz7ZU0Ymsu8G9JTwJ7SboNWBIvQ7ygsdLM/lIZpXLOuVpLUKdyHtRYATSN2T4Y+KZYmvbA+DBg7AucIWmbmf2zgtcsnBr9cuBTgsrCpfEyxAsae87AY+ecq6By1jTimQMcIak58DXBYkgXxiYws+ZF15VGA/+OEDAws+7lzRNvdHGPihbEOef2JJUx5NbMtgFXEoyK+hh43sw+lDRY0uCqKHdF1tMotaZhZnmVXUDnnKuNKuuBcDN7FXi12L5HSkl7SSVcsi9wd8x6Gk8SDLvtWFqGPeg5Ruecq3wi+CJN5JWEtpiZAUXraQAN4mVI0vtwzrkUoWDuqUReSSh2PY2p4b5o62k455wrnYD05AwIiaj69TScc87tKlVDRkXW0/DmKeeci0hK7JVsJL1cOFpK0khJCyT1jpfHg4ZzzkWSWH9GkvZpHG5mn0r6GXAy8FvgtngZvHnKOeciKBw9laK2hz+7Ay+a2SxJ2+Jl8KDhnHMRJbJWRpJaLelPwEXAuQqqQ3HjQgoHSOecSwKpPeT2MqAZMNLMPgQyCZ5KL5XXNFy5rZ7zYE0XodbLOT7u/7cuiaRy85SZfQEMjtleD0yPl8eDhnPORZSktYgySXqTEkYMm1k3SY+Z2eXFj3nQcM65iFIzZAAwIs6x0SXt9KDhnHMRpWhFo3CCxNKOzSppf6o2xTnnXFIonEYkkVeykHS0pPqSDpb0oqQfJa0K3/8kXl4PGs45F4kS/i+JPA1sBcYA84Cjwtf88FipvHnKOeciSqJKRKIUrjPe2Mzuitl/p6Q+8TJ6TcM55yIIhtwqoVcSqRMuvPSJpKJ1ySU1A5bGzVjVJXPOuVotSScjLMM9wLvAQmBROPQWgmW+/xsvowcN55yLKNWChpk9KWkG0IFdl5d9o6y8HjSccy6CVF2Eycw+Az4rbz4PGs45F1GSjYxKmKQnKfmJ8AGl5fGg4ZxzEaVgRaPQ3Jj39YFfAB/Gy+BBwznnIkrVmoaZPRy7LelvwGvx8njQcM65CASkpWbMKE3TeAc9aDjnXBRSyi7CVKxPIx1oC7wVL48HDeeciyg1Qwawa5/GNmCMmU2Jl8GDhnPORRA0T6Vm2Cjep5EIn0bEOeciUoKvZCMpS9Jjkr4LX49JahgvjwcN55yLKlWjBgwDdgAnACuBaQRTjJTKm6eccy6iVB1yC+QCx5jZDklmZuMk/S5eBg8azjkXUQoPuTUz21G4oWCx8/rxMnjzlHPORZW6zVMFkvYJ3zcAxgFT42XwmoZzzkUQxIPkjAgJuBpoCKwC/kkwgeGT8TJ40HDOuShScz0NAMzsLYBwxNQdZraurDzePOWccxFVVuuUpJ6SFktaIumGEo73lbQwfL0l6ZhI5ZZaS3oX+A74QdJcSa3j5fGg4ZxzUVVC1JCUDjwE9AJ+CvSR9NNiyb4AuphZG+A2YFTEkj8F3G9me5lZfeC+cF+pPGg451wkwdxTibzK0AFYYmZLzWwLMB7oHZvAzN4ys9Xh5jvAwRELX8fMxsWc/xnK6LbwoOGccxEkWslIoHmqCbA8ZntFuK80lwH/qUCRY82T1KFwQ9IJwMfxMnhHuHPORZV4R/i+kmInCRxlZoVNTCWdxUq8nNSNIGh0SvjKJfsp8JakReH20cAcSVMBzKxb8QweNJxzLqJyDLn90czal3JsBbuuZXEw8M1u15LaAI8DvcxsVXnKWYK7ypvBg4ZzzkVUSUNu5wBHSGoOfA1cAFy463XUDHgZ6Gdmn0a9oJm9Wt483qdRycaOGU3X3I5063wy782fv8uxgoICLunXlx5dc7mkX18KCgoA+OrLL+l5ane6dT6ZYXffCcCGDRvodVoPOp3UgYXvvw/AooULufXmP1fvDSWheJ/xyBHDyO14At06n8w1V/0Os6B2759xYq7u25kXRvTn2bsvotWh+1O/Xh0e+tP/8OzdF/H3G8+hYWa93fJ0aX8Y/7xvABOG9ePe63uTHs6p0bldC14c2Z8XR/Ynt20LAFo135+X772EZ+7qS4N6dQHod1a7ouMpKXxOI5FXPGa2DbgSmETQr/C8mX0oabCkwWGym4B9gIclLSjW1FUtqiRoSMqWdHH4/hZJF1XFdZLN6tWrefjBB5g8ZRpPjXmGIdf8fpfjY8eMpmWrVkyZNoMjW7Zk7JjRANz4vzdw4823MnX6LKZNfZPFn3zCG69Pplv3HgwbcS9jRgcPaN4zYhjXDd1t6PYepazPuHfvXzLjrdlMnT6L77//jmlT3wT8M05E6xYH0KblTzj3ujEMGfF//HnQqfTpeRyLPlvJhTc8w7+nf8jAX520W75r+3Xhijtf4vyhY9m6bTud2rYgLU3ccGl3Btw0ngE3jeePl3UnLU2ce9ox3D7qdd5a8CW5bVuQ3bABrVscwIz5S2vgjiuPEvyvLGb2qpkdaWaHmdkd4b5HzOyR8P2vzSzHzI4NX6U1dVWZqqppZAMXJ5pYUq2o8cx5dzYdO+WSkZHBoc2bs2H9ejZv3lx0fPr0afQ64ywAzjjzbGbOnA7AwvcX0KlTLgA9e53JzBnTyczMpKCggE2bNpKVlcWE8c9xdu9fkJmZWf03lkTK+owPP+KIovcZdTOoUydogfXPuGzNmzTmgyUrAVj54zqaHphNi4P3YdFnwb73F3/DiW0O2S3fp1/9wN6ZwRx3DTPrk5e/kUN/0pjl3+azbsNm1m3YzPJv8znkoBw2FWylXkYdGtSry8aCLVx5wck8OH5m9d1kFRCVU9NIFVX1ZX0t0E7SNOBMoJukV8LqVCsASdMkjZQ0iaAd73FJUyXNLBwCJuloSW9IelPS85IaVFF5K0VeXh45OTlF23s3akReXl7R9uqY49nZ2eStCvqwduwommQy2J+3iu49TmHjxo2Mf3YcF/cfwBuTJ9G0aTOGXHMVD9x3bzXdUfIp6zMuNP2/0/j225V0yu0M+GeciE+//IETjz6EunXSaNV8fw7cd2+++WEtndsdBkC34w8nu+HuE6D+Y8oiRt/WhzdGDWbb9u0s+mwl2Q3rk79+U1GatRsKyG7YgNGvzOGX3Y8mo246a9cXsCp/IycefQg3Xn4KXdsfVm33WtlSdb5CSemSjpPUJeb1gaSuknb/C4GqCxr3APPMrCswEVhnZj8nWPDj1zHp5prZ6UA3godaugG/Agr/j30IuNTMugOzCIaY7UbSwPDx97k//PhDldxQIho3bsyaNWuKttfm59O4ceOi7ZyY4/n5+eSEx9LSdv4z5Ofnk5PTmLS0NO4eNoLHnhzNs+PGct3QG7jjtlu466/DWfLZp3y+ZEm13FOyKeszhqBf4s//+0fGPjsBhX/e+WdctiXLf+SVaR/y9B0XMqB3Bz776geeeHk29TLqMO6uvhywT0O+y1u/W77bf9eLX179JKcMfIT8dQX06tSKNesK2DtrZ4BpmFmPNes28ePqDQy999/c9cQU+p3dnmdfnc/pHVtx+2NvcNkvT6jO261cqRo14B8EDxEOj3kdGv48raQM1dUsNC/8uYygE6fQW+HPo4Hzw5rJBKBRuP9nwNPh/j7AgSWd3MxGmVl7M2u/3777VXLRE3d8hxN4e9ZMtm7dyrJly8jMyqJevZ0dh7m5XZj0WjBYYdJrr5Kb2wWAo9scw9tvBR/F5En/KfrrGODzJUswM1q2akVeXh5mxubNm1m3rsx5xWqlsj7jz5csYfDll/L0uPHsu+++Rfv9M07MMxPn0ecPz/DEP2az+Mvv2bJtO7f8fRJ9/ziOFd/l89rMT3bLs2OHkb8+GNSxKn8j2Q0b8OU3eTQ9IJusBhlkNcig6QHZfLVydVGeX3Y/mn9P/wgDMvfKACB776RuSIirsvo0asChZtbSzDoUvoBPzex4M3uspAxVNeR2S7Fzxz6gEvvJbQ9/fkhQ07gXQFJGuP8DoI+ZrSy2Pynl5OQwcPAVnNq9C5IYcc/9vL9gAVOmvM61Q66nX/9LGHT5pfTomkuTgw9m1OPBFC+33X4XgwdexpYtWzi9Zy9atd45X9i9I4dz9/CRAAwafEVR3mOOPbYmbrHGlfUZXz/katbkr+HyS/sDcM2Q6+l1xpn+GSdozO19SE9PY83aTdz88Gsc3nRf/vLbnuzYsYNPvvieu56YAsCvTmnDd6vWMfO9Lxj59DTG3XURm7duY+36Ah594S127DCGj57K6Nv7ADB89FR27Ai+BjIbZNC2dRP+/NBrACxdvoqX7rmE/8yI+yByUkvhRZh2/ysA4laxVTgksTKFHdsTgY3A/sCjZvaMpE7Ar83skrD2cJGZrZBUF/gb0DI8xVwzu17SUcBIoG64/y4zez3etdu1a2+zZlf7KDTnKlXO8VfWdBH2CAULHpoXdQTSUce0tZcnJ9aZ3/LAzMjXq2zh928rgj/uF5vZ1njpq6SmES4f2KuE/TOBmeH7rjH7twKDS0j/AXB6VZTROecqQyovwiSpPfAisJngVupJOsfM5pSWx58Id865KFJ7OO0DQH8z+y8UzWl1P9CxtAweNJxzLqLUjRnsVRgwAMxsqqS94mWoFQ/VOedczRFSYq8ktCGsXQAgqTuwIV4Gr2k451xEyRkPEvI74CVJ2wg6wusRPCtXKg8azjkXQfI+t1c2M5sv6QjgSILbWBxOnFgqDxrOORdVqkYNimbX/SjR9B40nHMuolQdclsRHjSccy6iFO7TKDcPGs45F4VSehqRcvMht845F1lqTnMrqZGkJyR9J+l7SU9K2jteHg8azjkXQYovwnQfsB5oBxwHrGPn0hQl8uYp55yLKDnjQUKON7OjYravkrQwXgYPGs45F1GS1iISUdKMtttL2FfEm6eccy6iFF6E6b+SihbGk9QYmBEvg9c0nHMuolStaZjZ1cW284Dfx8vjQcM55yJI4k7uMkm6Od5xM7u1+D4PGs45F1GSNj0lIrO8GTxoOOdcVCkaM8xsaHnzeEe4c85FlJqP9oGkYyW9KOlxSftLypR0VLw8HjSccy4SkabEXkloLPBfIA8YCWwBHo6XwZunnHMugsInwlPURjP7m4JlBd83s62+3KtzzrnSfC7pKDMzYIekTKB+vAxe03DOuYhSuKaRA7wraQbQDHgXeDReBg8azjkXUQoPuX0ufAE8QdBEtTheBg8azjkXRQo/3AeMB7aZ2Y5EM3ifhnPORZDiU6O/ARwKIOklSWskDYyXwYOGc85FlMITFjYys6WS2gMNgZ8BV8fL4M1TzjkXUZLWIhJh4c/uwCtm9rWkgngZvKbhnHMRVdYT4ZJ6SlosaYmkG0o4LkkPhMcXSmobsejLJI0CrgAmSqpLGXHBg4ZzzkVVCVFDUjrwENAL+CnQR9JPiyXrBRwRvgYCf49Y8v7AUmCQmX0BpAPnxcvgzVPOORdRJfVXdACWmNlSAEnjgd7ARzFpegNPhw/jvSMpW9JBZraygtc8FHjMzFZJ2htoAbwfL0OtCxrz58/7sUFdfVXT5SinfYEfa7oQtZx/xtUj1T7nQ6Ke4L358ybtlaF9E0xeX9LcmO1RZjYqfN8EWB5zbAVwQrH8JaVpAlQ0aDwGnCIpA5gH7ACmEDRXlajWBQ0z26+my1BekuaaWfuaLkdt5p9x9dgTP2cz61lJpyqpumIVSFMe6Wa2RtJpwHQzu0zSR/EyeJ+Gc84lhxVA05jtg4FvKpCmPOpISgNOAaaG+zbHy+BBwznnksMc4AhJzcPmoguAV4qleQW4OBxFdSKQH6E/A+A1YBHQF/i3pEbA+ngZal3zVIoaVXYSF5F/xtXDP+cKMrNtkq4EJhGMYnrSzD6UNDg8/gjwKnAGsATYCAyIeM3rJb0ELDWzNeHu3Hh5FHTCO+ecc2Xz5innnHMJ86DhnHMuYR40nHPOJcyDhtsjhGsgl7rtnEuMBw1X60lKMzOTVF9SfYBw23//q0hJn60H6trBR08lCUk5wFHAAmBDeVbScqWTpDBANAGeBj4jWEOgT+zxGi1kLRMG6R2SDgC6Ap8AX5jZ2potmasM/pdWEpDUFHgZOAcYA3T3v4IrRxgw9gIeIJjobTCQLun5wuM1WsBaKAwYTYCngNbAlcDl4SyuLsX5F1MNC4PDb4DbgDsJVs76gmjzybiQpAwz2wisJahlYGbnAevDWT1d1bgYeITgj6BjCB5Ky/TAkfo8aCSP3sATBLWNQ4Db/H+wigunWcgAhkjqRDCD50mSjpd0NtCyZktYu5RQM94MnEpQwxsI7A/8BahfzUVzlcyDRg2RdICkXKARMBboSFDDyAD+BDxnZttrsIgpKaaztYGZbQnf7w28SFB7u5bgS+xyb2OvHDF9GAdJOi38vX6CYAnRLwnWnr6RYBrwDTVYVFcJvCO8BkjaBxgPbAAWA+8CnwIXAg0IFkX5sOZKmNrCPoxZBKua5QFDgf5m9rGkBsBeZraqJstY20g6EHiJIFj8AbgdmEHQTJUGPG9mcafcdqnBg0Y1C0dJXQd8aWaPSepDMGpqlpm9KindaxgVJ6lOOPHb3wgC8HPAMOBj4Doz+7ZGC1iLxNQw0oFbCWoVzwH/IQgc82Jqe66W8OapaiSpDtCOYERJHUn1CP4HWwKcICnLA0b5STpG0lHh5/uypM4E00w3JAgWzwP7AQU1WMxaJSZgHEhQQ15IsK71ZOBSgllaR/lgg9rHp0avJpIOJmguWUQQNL4AOhFU4V8kqPXFncfelWoLQbNIHYL1Ac4ElhGsd/xLM/urpFExUz+7iMKAsQ/ByL9lBAMNLiBoaj0W+C3wG+83qn08aFQDSQ0JRpH8g+Cv3lbALwj++q1rZq/VXOlqhcXA1wTB+HmC2sVhwLkE6x8/YWara7B8tUZhDSPcvJIgQP/azD6S9BDQmKA2PcjMPq2pcrqq430a1UBSNvA48Ccz+zScyuIO4C3gbTOLslyjA8IVx34G3ELQCVtY01hiZstqsGi1TtiMuj58fydwIHCFmRWE+/wp+1rMg0Y1CMewXw+sI1iV6yiCv9LOMrO46/G68pF0GnAzwfDa8zwgVw5JFwBzgdXAv8L3n5rZg5JGAE2AgWa2zoNG7eZBo5qEU4VcBLQnGNVzvQ+rrRph/5GZ2dc1XZbaQNJBwFXAGuAnBPOjzSXo8P7CzO6XdAfwNx+dVvt50KhG4eiebCDNzL6v4eI4V6ZwJNrnQBbBYIPvgLvNbI6k1gTDx+eZ2cM1WExXjTxoOOdKJemnBMGibvhzH2Ar8E8zWyypJbDGzL6rwWK6auTPaTjn4vmEYGRaPeBt4G+AgIskHWpmiz1g7Fk8aDjnShUOr70MGAQMJxjK/BXBsFp/rmgP5M1TzrmESDqdYGTaj8C1ZrakhovkaoAHDedcwsJRgDt8ZNqey4OGc865hHmfhnPOuYR50HDOOZcwDxrOOecS5kHDOedcwjxouCohabukBZI+kPRCuARrRc81WtI54fvHw6eUS0vbVVLHClzjS0n7Jpj2EkkPlvcaztUGHjRcVdlkZsea2VEEiyQNjj0YLhFabmb26zLWmu4KlDtoOOcS40HDVYcZwOFhLWCqpGeBRZLSJQ2XNEfSQkmDIFiPQdKDkj6SNBHYv/BEkqZJah++7ylpvqT3JU2RdChBcLomrOXkStpP0kvhNeZIOjnMu4+kyZLek/QowdQYuyl+jRKOny1pdnieNyQdEO7vEpZhQXisoaSDJE2PqYHlVuqn7Fw18JX7XJUKZ/btRbAMK0AH4Cgz+0LSQCDfzI4P10ufJWkycBzQEjgaOAD4CHiy2Hn3Ax4DOofnamxmeZIeAdab2Ygw3bPAvWY2U1IzgvVMWhM82TzTzP4i6UxgYAll3+0aJdziTOBEMzNJvwaGAkMIZn/9rZnNkpRFsD75QGCSmd0R1rQq3GTnXE3xoOGqSgNJC8L3MwhmSO0IvGtmX4T7TwPaFPZXAI2AI4DOwHNmth34RtKbJZz/RGB64bnMLK+UcpwC/FQqqkjsHS6/2xn4nzDvREklLQebyDUOBiaEa05kEKz9DjALuEfSOOBlM1shaQ7wpKS6BLPELijhfM4lNW+eclWlsE/jWDP7nZltCfdviEkj4Hcx6Zqb2eTwWFlTFSiBNBD8jp8Uc40mZrauEq/xN+BBMzuaYFK/+gBmdjfwa4IFt96R1MrMphMEq6+BsZIuTqD8ziUVDxquJk0CfhP+5Y2kIyVlAtOBC8I+j4OAbiXkfRvoIql5mLew6Wgd0DAm3WSCpXUJ0x0bvp0O9A339QJyynGNWI0IggBA/5jrHGZmi8zsrwSr3LWSdAjwvZk9RlDzalvC+ZxLah40XE16nKC/Yr6kD4BHCZpM/wF8BiwC/g78t3hGM/uBoI/gZUnvAxPCQ/8CflnYEQ78HmgfdrR/xM5RXLcCnSXNJ2gmW1aOa8S6BXhB0gyC2V8LXR12dr8PbAL+QzCya4Gk94BfAfeX/RE5l1x8wkLnnHMJ85qGc865hHnQcM45lzAPGs455xLmQcM551zCPGg455xLmAcN55xzCfOg4ZxzLmEeNJxzziXs/wEcJyf7DQ0KxQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000    0    0]\n",
      " [   0 1000    0]\n",
      " [   0    1  999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1594336/1503990667.py:16: MatplotlibDeprecationWarning: The 'cmap' parameter to Colorbar has no effect because it is overridden by the mappable; it is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  cbar = plt.colorbar(heatmap, cmap=plt.cm.Blues, ax=ax)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1000    0    0]\n",
      " [   0 1000    0]\n",
      " [   0    1  999]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1594336/1503990667.py:16: MatplotlibDeprecationWarning: The 'cmap' parameter to Colorbar has no effect because it is overridden by the mappable; it is deprecated since 3.3 and will be removed two minor releases later.\n",
      "  cbar = plt.colorbar(heatmap, cmap=plt.cm.Blues, ax=ax)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Now we will plot the confusion matrices for each trained model\n",
    "#first get the predictions\n",
    "pred = model1.predict([traits_BM.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "classOrderLs=['one','two','three']\n",
    "\n",
    "# Print the confusion matrix\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions for the next dataset\n",
    "pred = model2.predict([traits_OU.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()\n",
    "\n",
    "# get the predictions for the next dataset\n",
    "pred = model3.predict([traits_disc.reshape(3000,-1), xtest])\n",
    "pred_cat = [i.argmax() for i in pred]\n",
    "ytest_cat = [i.argmax() for i in ytest]\n",
    "\n",
    "counts=[[0.,0.,0.],[0.,0.,0.],[0.,0.,0.]]\n",
    "for i in range(len(ytest_cat)):\n",
    "    counts[ytest[i]][pred_cat[i]] += 1\n",
    "counts.reverse()\n",
    "\n",
    "# Print the confusion matrix\n",
    "print (confusion_matrix(ytest, pred_cat))\n",
    "#now do the plotting\n",
    "fig,ax= plt.subplots(1,1)\n",
    "makeConfusionMatrixHeatmap(counts, \"Confusion matrix\", classOrderLs, classOrderLs, ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tensorflow-gpu-2.8.0_py3.9",
   "language": "python",
   "name": "module-conda-env-tensorflow-gpu-2.8.0_py3.9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
